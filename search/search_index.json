{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome OpenRZ is a Rappelz server development community based on freely sharing knowledge and assets. Originally founded by Aodai, Alucard, iSmokeDrow and Mustafa in 2020, this site aims to establish a long term repository of information releases for public consumption. Here you will be able to find: Documentation Guides Releases Discord If you have any questions or want your Documentation/Guide/Release featured here please contact iSmokeDrow/Aodai on the discord via #lobby or through a dm.","title":"Home"},{"location":"#welcome","text":"OpenRZ is a Rappelz server development community based on freely sharing knowledge and assets. Originally founded by Aodai, Alucard, iSmokeDrow and Mustafa in 2020, this site aims to establish a long term repository of information releases for public consumption. Here you will be able to find: Documentation Guides Releases Discord If you have any questions or want your Documentation/Guide/Release featured here please contact iSmokeDrow/Aodai on the discord via #lobby or through a dm.","title":"Welcome"},{"location":"credits_thanks/","text":"Third Party Moonsharp HexBox mkdocs Special Thanks Aodai (Co-Founder) Alucard (Co-Founder) Glandu2","title":"Credits/Thanks"},{"location":"credits_thanks/#third-party","text":"Moonsharp HexBox mkdocs","title":"Third Party"},{"location":"credits_thanks/#special-thanks","text":"Aodai (Co-Founder) Alucard (Co-Founder) Glandu2","title":"Special Thanks"},{"location":"documentation/","text":"Source Server Mix Resource Pet Resource Client Tools DataCore Daedalus Grimoire NFMapEdit","title":"Documentation"},{"location":"documentation/#source","text":"","title":"Source"},{"location":"documentation/#server","text":"Mix Resource Pet Resource","title":"Server"},{"location":"documentation/#client","text":"","title":"Client"},{"location":"documentation/#tools","text":"DataCore Daedalus Grimoire NFMapEdit","title":"Tools"},{"location":"guides/","text":"Server Mix Category Resource","title":"Guides"},{"location":"guides/#server","text":"Mix Category Resource","title":"Server"},{"location":"releases/","text":"Server Name Link Description Author Remove Deleted Characters link T-SQL script that removes deleted characters and their assets. iSmokeDrow Client Tools Name Link Description Author Grimoire link Multi-use Rappelz asset manager iSmokeDrow NFMapEdit link Proprietary map editor. Retail","title":"Releases"},{"location":"releases/#server","text":"Name Link Description Author Remove Deleted Characters link T-SQL script that removes deleted characters and their assets. iSmokeDrow","title":"Server"},{"location":"releases/#client","text":"","title":"Client"},{"location":"releases/#tools","text":"Name Link Description Author Grimoire link Multi-use Rappelz asset manager iSmokeDrow NFMapEdit link Proprietary map editor. Retail","title":"Tools"},{"location":"documentation/daedalus-doc/","text":"Daedalus Documentation Override List Overrides are LUA variables that can be defined to change Daedalus behaviours during loading/saving operations. Overrides: header specialCase sqlColumns selectStatement Field Types The FIELDTYPE identifier must ALWAYS be ALL CAPS and any typos will result in program errors len is types length in bytes all fieldtypes can present a default value. all fieldtypes can set their own show (visibility) Can be 0 or 1 all fieldtypes can present a flag , advanced use! Type Len Args Description BYTE 1 int length Reads 1 byte from the stream, otherwise reads length bytes from the stream. (reading multiple bytes is generally used for padding ) BIT_VECTOR 4 n/a Reads 4 bytes from the stream into a BitVector32 for per bit processing. BIT_FROM_VECTOR n/a int bit_position, string dependency This fieldtype is not read from the stream, the value is obtained by reading the n bit of the value stored by dependency . (must be an int ) INT16/SHORT 2 n/a Reads 2 bytes from the stream as an short UINT16/USHORT 2 n/a Reads 2 bytes from the stream as an ushort INT32/INT 4 n/a Reads 4 bytes from the stream as an int UINT32/UINT 4 n/a Reads 4 bytes from the stream as an uint INT64/LONG 4 n/a Reads 8 bytes from the stream as an long UINT64/ULONG 4 n/a Reads 8 bytes from the stream as an ulong SINGLE 4 n/a Reads 4 bytes from the stream as an single FLOAT 4 n/a Reads 4 bytes from the stream as an single DOUBLE 8 n/a Reads 8 bytes from the stream as an double FLOAT64 8 n/a Reads 8 bytes from the stream as an double DECIMAL 4 n/a Reads 4 bytes from the stream as an decimal DATETIME 4 n/a Reads 4 bytes from the stream as an int (seconds since epoch) which is stored as an DateTime SID n/a n/a This fieldtype is not read from the stream, the value starts at 1 and is incremented for every row read from the rdb. STRING n/a int length Reads length bytes from the stream and encodes them into a string according to currently selected Encoding STRING_LEN 4 n/a This fieldtype reads a 4 byte int from the stream and is later used to provide the length for a STRING_BY_LEN cell. STRING_BY_LEN n/a string dependency This field type gets its length from a previously defined STRING_LEN cell by the name of dependency and is then read and encoded according to currently selected Encoding STRING_BY_REF n/a string dependency This field type gets its length from a header cell of type int Dependency Note: Usage: { \"fieldName\", STRING_BY_REF, dependency=\"headerFieldName\" } Description: Fields marked with a dependency means they rely on another field Possible Flags: ROWCOUNT (used in header definition to denote engine loop count) LOOPCOUNTER (used in specialCase=DOUBLELOOP files) Defining Field Lists There are two types of lists that can be defined in a structure.lua, those are: - header (All fields in this list will be used to process a files header) - fields (All fields in this list will be used to process a files contents) Defining Header Normally the header of any given .rdb is as follows: Date 8 bytes Blank 120 bytes Row Count 4 bytes In this case, you will simply NOT define a header! However in special cases (e.g. .ref files) the header is different, which will cause an issue with Deadalus reading this file, to fix this we can now simply script our header by defining a field list as we would below, but giving it the name header . See the example below: The header list is a lua table containing tables of information header = { { \"rows\", INT32, flag=ROWCOUNT }, -- flag=ROWCOUNT identifies that the engine will loop on the INT value stored in \"rows\" { \"strLen\", INT32, default=52 } -- Holds the length of fields.string } Notice that in the case of db_item.ref , this file does not have the date/blank areas but instead contains two INT32 fields, one of which would define the row count as a normal header. In order for the engine to work properly we must inform it that the \"rows\" field contains the row count by giving it the variable flag with the value ROWCOUNT Defining Basic Fields The fields list is a lua table containing tables of information fields = { { \"fieldName\", FIELDTYPE }, { \"fieldName\", FIELDTYPE } } fieldName is a string which will be used as an identifier when the rdb data is displayed in the calling tool FIELDTYPE is an identifier that determines how many bytes will be read on the particular field while processing the rdb. Defining Complex Fields While some rdb have easily definable fields some need extra information to process correctly, refer to the examples below. db_string db_string structure example: fields = { { \"name\", STRING_LEN }, { \"value\", STRING_LEN }, { \"name\", STRING_BY_LEN }, { \"value\", STRING_BY_LEN }, { \"code\", INT32 }, { \"group_id\", INT32 }, { \"unknown\", BYTE, length=16, default=0, show=0 } } As you can see above we make use of the STRING_LEN / STRING_BY_LEN fieldtypes's but we also need to process 'useless' data in order to read the rdb properly. So we define the 'unknown' field. Sometimes we need to process data that we don't want to show, in this case we will use the show variable and set it to 0 In this particular case we need to read a specific amount of bytes to properly process the 'unknown' field, while this could be done as: fields = { { \"unknown1\", INT32 }, { \"unknown2\", INT32 }, { \"unknown3\", INT32 }, { \"unknown4\", INT32 }, } it is accomplished much easier by using the fieldtype BYTE and defining the length variable as 16 In this particular case we also define the default variable, this variable is used when saving an rdb. For instance, had we loaded this rdb's information from an SQL table we have no proper way of knowing what to place in this area of useless data. Thusly, if we already know there is no information here we can provide a default value of 0 . NOTE: FIELDTYPE: STRING can also have a default string value! db_item/db_monster There are however even more complex field definitions required by some rdb, in the next instance we will look at using the fieldtype BIT_VECTOR and BIT_FROM_VECTOR as they are used in db_item / db_monster In db_item there are limit_* fields such as limit_deva , limit_gaia , limit_asura which limit the ability of races or classes to use that specific item. See the example below: fields = { {\"limit_bits\", BIT_VECTOR, show=0 }, {\"limit_deva\", BIT_FROM_VECTOR, dependency=\"limit_bits\", bit_position=2}, {\"limit_asura\", BIT_FROM_VECTOR, dependency=\"limit_bits\", bit_position=3}, {\"limit_gaia\", BIT_FROM_VECTOR, dependency=\"limit_bits\", bit_position=4}, {\"limit_fighter\", BIT_FROM_VECTOR, dependency=\"limit_bits\", bit_position=10}, {\"limit_hunter\", BIT_FROM_VECTOR, dependency=\"limit_bits\", bit_position=11}, {\"limit_magician\", BIT_FROM_VECTOR, dependency=\"limit_bits\", bit_position=12}, {\"limit_summoner\", BIT_FROM_VECTOR, dependency=\"limit_bits\", bit_position=13} } In this particular instance we will be reading an INT from the physical rdb and converting it into individual values, in c++ this would be done by reading the individual bits of this section of the rdb. Unfortunately this can not be accomplished in that manner in c#, so we read the INT and convert it into a BitVector32 which we can then read by position. So in the above example we read the INT { \"limit_bits\" } and convert it into a BitVector32 by giving this cell the fieldtype BIT_VECTOR , the fields with the variable dependency of the same name limit_bits are not physically read from the rdb but are generated from the limit_bits field by reading the bit at the position given by the bit_position variable. So in the above example limit_magician will generate it's value by reading the bit at position 12 in the limit_bits fields. Another unique configuration of fields in db_item is repeated or duplicated fields such as item_use_flag which appears twice in a row in this rdb. See the example below: Example 1: fields = { {\"item_use_flag\", INT32}, {\"item_use_flag\", INT32, show=0} -- Use the same name so engine will copy shown value } If we were to load the physical rdb in this case we could define it like so: Example 2: fields = { {\"item_use_flag\", INT32}, {\"item_use_flag2\", INT32 } } and all would be good, unfortunately if we use the above structure and attempt to load the data from the ItemResource sql table we will be prompted by a column does not exist error. In this instance we can either simply use the engines duplicated field feature by defining the structure like in Example 1 , which will automatically give the field with the variable show set to 0 the same value as the shown field. Or we can go a little more complex by defining the value in a special LUA variable called selectStatement , in this case you would simply define the value as part of the sql statement like below: selectStatement = \"SELECT item_use_flag, item_use_flag as item_use_flag2 FROM dbo.ItemResource\" db_skilltree In the case of db_skilltree we can not read this rdb as we would the others, this is because unlike other rdb who use a single read loop based on a variable called the row count (which is part of the file header) this particular rdb uses a double read loop, where its initial loop is based off the count in the header and then a secondary count based off a field in the rdb itself. Consider the below structure example: specialCase = DOUBLELOOP fields = { { \"job_id\", INT32, flag=LOOPCOUNTER }, { \"skill_id\", INT32 }, { \"min_skill_lv\", INT32 }, { \"max_skill_lv\", INT32 }, { \"lv\", INT32 }, { \"job_lv\", INT32 }, { \"jp_ratio\", SINGLE }, { \"need_skill_id_1\", INT32 }, { \"need_skill_lv_1\", INT32 }, { \"need_skill_id_2\", INT32 }, { \"need_skill_lv_2\", INT32 }, { \"need_skill_id_3\", INT32 }, { \"need_skill_lv_3\", INT32 }, { \"cenhance_min\", INT32 }, { \"cenhance_max\", INT32 } } In this rdb structure first and foremost we need to alert the engine to the fact that this particular rdb is a special case, we will do this by defining the LUA variable specialCase In this instance our special case will be DOUBLELOOP . Once this has been defined the engine will attempt to do a secondary loop during its initial read loop, this however requires the engine know which field it will be using as the count for the secondary loop. This is where the flag variable will come into use. By defining the flag variable as LOOPCOUNTER . You are not only telling the engine to use this field during the read process but also to count the amount of rows in loaded data that have the same job_id or flag=LOOPCOUNTER so that it can write this count back down when you are saving this particular file. NOTE: at this time there is only one specialCase and it is DOUBLELOOP Using LUA ProcessRow In the case of db_monster we finally have to do some row processing with our trusty ProcessRow function, in the case of this particular rdb the id field has had it's bits scrambled when it was written. This means if you just load the rdb as normal you will not get the proper id. In order to counteract this we will need to perform some bitwise operations on this field for each row as they are loaded. For such cases Daedalus already has a built in feature. Simply define the ProcessRow function. Consider the below example: local encodeMap = {} local decodeMap = {} function compute_bit_swapping() local j, oldValue for i = 0,31 do encodeMap[i] = i; end j = 3 for i = 0,31 do oldValue = encodeMap[i] encodeMap[i] = encodeMap[j] encodeMap[j] = oldValue j = (j + i + 3) % 32 end for i = 0,31 do decodeMap[encodeMap[i]] = i end end compute_bit_swapping() ProcessRow = function (mode, row, rowNum) local value = row[\"id\"] local result = 0 if mode == READ then for i = 0,31 do if bit32.band(bit32.lshift(1, i), value) ~= 0 then result = bit32.bor(result, (bit32.lshift(1, decodeMap[i]))) end end elseif mode == WRITE then for i = 0,31 do if bit32.band(bit32.lshift(1, i), value) ~= 0 then result = bit32.bor(result, (bit32.lshift(1, encodeMap[i]))) end end end row[\"id\"] = result end Notes: The ProcessRow function must always present the following arguments (mode, row, rowNum) Any function call like compute_bit_swapping() will occur automatically and doesn't need to be called manually within ProcessRow If ProcessRow has been defined it will be called once per row read / write operation In this above example we provide the encode/decode maps by defining the tables and computing the values they will hold by calling compute_bit_swapping() so when ProcessRow is called by the engine it will already have access to these variables. Now you can alter the current row being processed by Daedalus because it was passed into the ProcessRow function as the row argument. In this case we need to perform the bitwise operation on the field id so we define the local value as row[\"id\"] , here you can use strongly typed names as the key or you can provide the oridinal value which in this case would be row[0]. Then at the end of this operation we set the row[\"id\"] value to the newly decoded value and now we have the proper id when displayed. SQL The last topic we will be covering is the proper usage of the LUA variables sqlColumns and selectStatement These two variables should only ever be defined in the case that your provided structure (e.g. fields) can not result in a proper insert or select statement. For instance if you are selecting data which needs to be combined from two tables in the instance of db_skill or inserting data from a rdb whose epic or structure does not match the epic or structure of the target table. Example sqlColumns for db_npcresource (for 7.3): -- To send rdb data to a database table of different epic you will need to define -- the sql table structure below, in this instance the data is going backwards to 7.2 -- table. If you are sending information between matching epics, do not define sqlColumns sqlColumns = { \"id\", \"text_id\", \"name_text_id\", \"race_id\", \"sexsual_id\", \"x\", \"y\", \"z\", \"face\", \"local_flag\", \"is_periodic\", \"begin_of_period\", \"end_of_period\", \"face_x\", \"face_y\", \"face_z\", \"model_file\", \"hair_id\", \"face_id\", \"body_id\", \"weapon_item_id\", \"shield_item_id\", \"clothes_item_id\", \"helm_item_id\", \"gloves_item_id\", \"boots_item_id\", \"belt_item_id\", \"mantle_item_id\", \"necklace_item_id\", \"earring_item_id\", \"ring1_item_id\", \"ring2_item_id\", \"motion_id\", \"is_roam\", \"roaming_id\", \"standard_walk_speed\", \"standard_run_speed\", \"walk_speed\", \"run_speed\", \"attackable\", \"offensive_type\", \"spawn_type\", \"chase_range\", \"regen_time\", \"level\", \"stat_id\", \"attack_range\", \"attack_speed_type\", \"hp\", \"mp\", \"attack_point\", \"magic_point\", \"defence\", \"magic_defence\", \"attack_speed\", \"magic_speed\", \"accuracy\", \"avoid\", \"magic_accuracy\", \"magic_avoid\", \"ai_script\", \"contact_script\", \"texture_group\", --\"type\" <this field does not exist in 7.2> } Example select for db_skill: -- Use DISCTINT keyword to avoid duplicates left by gala selectStatement = \"SELECT DISTINCT [id],[text_id],[is_valid],[elemental],[is_passive],[is_physical_act],[is_harmful],[is_need_target],[is_corpse],[is_toggle],[toggle_group],[casting_type],[casting_level],[cast_range],[valid_range],[cost_hp],[cost_hp_per_skl],[cost_mp],[cost_mp_per_skl],[cost_mp_per_enhance],[cost_hp_per],[cost_hp_per_skl_per],[cost_mp_per],[cost_mp_per_skl_per],[cost_havoc],[cost_havoc_per_skl],[cost_energy],[cost_energy_per_skl],[cost_exp],[cost_exp_per_enhance],[cost_jp],[cost_jp_per_enhance],[cost_item],[cost_item_count],[cost_item_count_per_skl],[need_level],[need_hp],[need_mp],[need_havoc],[need_havoc_burst],[need_state_id],[need_state_level],[need_state_exhaust],[vf_one_hand_sword],[vf_two_hand_sword],[vf_double_sword],[vf_dagger],[vf_double_dagger],[vf_spear],[vf_axe],[vf_one_hand_axe],[vf_double_axe],[vf_one_hand_mace],[vf_two_hand_mace],[vf_lightbow],[vf_heavybow],[vf_crossbow],[vf_one_hand_staff],[vf_two_hand_staff],[vf_shield_only],[vf_is_not_need_weapon],[delay_cast],[delay_cast_per_skl],[delay_cast_mode_per_enhance],[delay_common],[delay_cooltime],[delay_cooltime_mode_per_enhance],[cool_time_group_id],[uf_self],[uf_party],[uf_guild],[uf_neutral],[uf_purple],[uf_enemy],[tf_avatar],[tf_summon],[tf_monster],[skill_lvup_limit],[target],[effect_type],[state_id],[state_level_base],[state_level_per_skl],[state_level_per_enhance],[state_second],[state_second_per_level],[state_second_per_enhance],[state_type],[probability_on_hit],[probability_inc_by_slv],[hit_bonus],[hit_bonus_per_enhance],[percentage],[hate_mod],[hate_basic],[hate_per_skl],[hate_per_enhance],[critical_bonus],[critical_bonus_per_skl],[var1],[var2],[var3],[var4],[var5],[var6],[var7],[var8],[var9],[var10],[var11],[var12],[var13],[var14],[var15],[var16],[var17],[var18],[var19],[var20],[jp_01],[jp_02],[jp_03],[jp_04],[jp_05],[jp_06],[jp_07],[jp_08],[jp_09],[jp_10],[jp_11],[jp_12],[jp_13],[jp_14],[jp_15],[jp_16],[jp_17],[jp_18],[jp_19],[jp_20],[jp_21],[jp_22],[jp_23],[jp_24],[jp_25],[jp_26],[jp_27],[jp_28],[jp_29],[jp_30],[jp_31],[jp_32],[jp_33],[jp_34],[jp_35],[jp_36],[jp_37],[jp_38],[jp_39],[jp_40],[jp_41],[jp_42],[jp_43],[jp_44],[jp_45],[jp_46],[jp_47],[jp_48],[jp_49],[jp_50],[desc_id],[tooltip_id],[icon_id],[icon_file_name],[is_projectile],[projectile_speed],[projectile_acceleration] FROM dbo.SkillResource s LEFT JOIN dbo.SkillJpResource sj ON sj.skill_id = s.id\"","title":"Daedalus Documentation"},{"location":"documentation/daedalus-doc/#daedalus-documentation","text":"","title":"Daedalus Documentation"},{"location":"documentation/daedalus-doc/#override-list","text":"Overrides are LUA variables that can be defined to change Daedalus behaviours during loading/saving operations. Overrides: header specialCase sqlColumns selectStatement","title":"Override List"},{"location":"documentation/daedalus-doc/#field-types","text":"The FIELDTYPE identifier must ALWAYS be ALL CAPS and any typos will result in program errors len is types length in bytes all fieldtypes can present a default value. all fieldtypes can set their own show (visibility) Can be 0 or 1 all fieldtypes can present a flag , advanced use! Type Len Args Description BYTE 1 int length Reads 1 byte from the stream, otherwise reads length bytes from the stream. (reading multiple bytes is generally used for padding ) BIT_VECTOR 4 n/a Reads 4 bytes from the stream into a BitVector32 for per bit processing. BIT_FROM_VECTOR n/a int bit_position, string dependency This fieldtype is not read from the stream, the value is obtained by reading the n bit of the value stored by dependency . (must be an int ) INT16/SHORT 2 n/a Reads 2 bytes from the stream as an short UINT16/USHORT 2 n/a Reads 2 bytes from the stream as an ushort INT32/INT 4 n/a Reads 4 bytes from the stream as an int UINT32/UINT 4 n/a Reads 4 bytes from the stream as an uint INT64/LONG 4 n/a Reads 8 bytes from the stream as an long UINT64/ULONG 4 n/a Reads 8 bytes from the stream as an ulong SINGLE 4 n/a Reads 4 bytes from the stream as an single FLOAT 4 n/a Reads 4 bytes from the stream as an single DOUBLE 8 n/a Reads 8 bytes from the stream as an double FLOAT64 8 n/a Reads 8 bytes from the stream as an double DECIMAL 4 n/a Reads 4 bytes from the stream as an decimal DATETIME 4 n/a Reads 4 bytes from the stream as an int (seconds since epoch) which is stored as an DateTime SID n/a n/a This fieldtype is not read from the stream, the value starts at 1 and is incremented for every row read from the rdb. STRING n/a int length Reads length bytes from the stream and encodes them into a string according to currently selected Encoding STRING_LEN 4 n/a This fieldtype reads a 4 byte int from the stream and is later used to provide the length for a STRING_BY_LEN cell. STRING_BY_LEN n/a string dependency This field type gets its length from a previously defined STRING_LEN cell by the name of dependency and is then read and encoded according to currently selected Encoding STRING_BY_REF n/a string dependency This field type gets its length from a header cell of type int Dependency Note: Usage: { \"fieldName\", STRING_BY_REF, dependency=\"headerFieldName\" } Description: Fields marked with a dependency means they rely on another field Possible Flags: ROWCOUNT (used in header definition to denote engine loop count) LOOPCOUNTER (used in specialCase=DOUBLELOOP files)","title":"Field Types"},{"location":"documentation/daedalus-doc/#defining-field-lists","text":"There are two types of lists that can be defined in a structure.lua, those are: - header (All fields in this list will be used to process a files header) - fields (All fields in this list will be used to process a files contents)","title":"Defining Field Lists"},{"location":"documentation/daedalus-doc/#defining-header","text":"Normally the header of any given .rdb is as follows: Date 8 bytes Blank 120 bytes Row Count 4 bytes In this case, you will simply NOT define a header! However in special cases (e.g. .ref files) the header is different, which will cause an issue with Deadalus reading this file, to fix this we can now simply script our header by defining a field list as we would below, but giving it the name header . See the example below: The header list is a lua table containing tables of information header = { { \"rows\", INT32, flag=ROWCOUNT }, -- flag=ROWCOUNT identifies that the engine will loop on the INT value stored in \"rows\" { \"strLen\", INT32, default=52 } -- Holds the length of fields.string } Notice that in the case of db_item.ref , this file does not have the date/blank areas but instead contains two INT32 fields, one of which would define the row count as a normal header. In order for the engine to work properly we must inform it that the \"rows\" field contains the row count by giving it the variable flag with the value ROWCOUNT","title":"Defining Header"},{"location":"documentation/daedalus-doc/#defining-basic-fields","text":"The fields list is a lua table containing tables of information fields = { { \"fieldName\", FIELDTYPE }, { \"fieldName\", FIELDTYPE } } fieldName is a string which will be used as an identifier when the rdb data is displayed in the calling tool FIELDTYPE is an identifier that determines how many bytes will be read on the particular field while processing the rdb.","title":"Defining Basic Fields"},{"location":"documentation/daedalus-doc/#defining-complex-fields","text":"While some rdb have easily definable fields some need extra information to process correctly, refer to the examples below.","title":"Defining Complex Fields"},{"location":"documentation/daedalus-doc/#db_string","text":"db_string structure example: fields = { { \"name\", STRING_LEN }, { \"value\", STRING_LEN }, { \"name\", STRING_BY_LEN }, { \"value\", STRING_BY_LEN }, { \"code\", INT32 }, { \"group_id\", INT32 }, { \"unknown\", BYTE, length=16, default=0, show=0 } } As you can see above we make use of the STRING_LEN / STRING_BY_LEN fieldtypes's but we also need to process 'useless' data in order to read the rdb properly. So we define the 'unknown' field. Sometimes we need to process data that we don't want to show, in this case we will use the show variable and set it to 0 In this particular case we need to read a specific amount of bytes to properly process the 'unknown' field, while this could be done as: fields = { { \"unknown1\", INT32 }, { \"unknown2\", INT32 }, { \"unknown3\", INT32 }, { \"unknown4\", INT32 }, } it is accomplished much easier by using the fieldtype BYTE and defining the length variable as 16 In this particular case we also define the default variable, this variable is used when saving an rdb. For instance, had we loaded this rdb's information from an SQL table we have no proper way of knowing what to place in this area of useless data. Thusly, if we already know there is no information here we can provide a default value of 0 . NOTE: FIELDTYPE: STRING can also have a default string value!","title":"db_string"},{"location":"documentation/daedalus-doc/#db_itemdb_monster","text":"There are however even more complex field definitions required by some rdb, in the next instance we will look at using the fieldtype BIT_VECTOR and BIT_FROM_VECTOR as they are used in db_item / db_monster In db_item there are limit_* fields such as limit_deva , limit_gaia , limit_asura which limit the ability of races or classes to use that specific item. See the example below: fields = { {\"limit_bits\", BIT_VECTOR, show=0 }, {\"limit_deva\", BIT_FROM_VECTOR, dependency=\"limit_bits\", bit_position=2}, {\"limit_asura\", BIT_FROM_VECTOR, dependency=\"limit_bits\", bit_position=3}, {\"limit_gaia\", BIT_FROM_VECTOR, dependency=\"limit_bits\", bit_position=4}, {\"limit_fighter\", BIT_FROM_VECTOR, dependency=\"limit_bits\", bit_position=10}, {\"limit_hunter\", BIT_FROM_VECTOR, dependency=\"limit_bits\", bit_position=11}, {\"limit_magician\", BIT_FROM_VECTOR, dependency=\"limit_bits\", bit_position=12}, {\"limit_summoner\", BIT_FROM_VECTOR, dependency=\"limit_bits\", bit_position=13} } In this particular instance we will be reading an INT from the physical rdb and converting it into individual values, in c++ this would be done by reading the individual bits of this section of the rdb. Unfortunately this can not be accomplished in that manner in c#, so we read the INT and convert it into a BitVector32 which we can then read by position. So in the above example we read the INT { \"limit_bits\" } and convert it into a BitVector32 by giving this cell the fieldtype BIT_VECTOR , the fields with the variable dependency of the same name limit_bits are not physically read from the rdb but are generated from the limit_bits field by reading the bit at the position given by the bit_position variable. So in the above example limit_magician will generate it's value by reading the bit at position 12 in the limit_bits fields. Another unique configuration of fields in db_item is repeated or duplicated fields such as item_use_flag which appears twice in a row in this rdb. See the example below: Example 1: fields = { {\"item_use_flag\", INT32}, {\"item_use_flag\", INT32, show=0} -- Use the same name so engine will copy shown value } If we were to load the physical rdb in this case we could define it like so: Example 2: fields = { {\"item_use_flag\", INT32}, {\"item_use_flag2\", INT32 } } and all would be good, unfortunately if we use the above structure and attempt to load the data from the ItemResource sql table we will be prompted by a column does not exist error. In this instance we can either simply use the engines duplicated field feature by defining the structure like in Example 1 , which will automatically give the field with the variable show set to 0 the same value as the shown field. Or we can go a little more complex by defining the value in a special LUA variable called selectStatement , in this case you would simply define the value as part of the sql statement like below: selectStatement = \"SELECT item_use_flag, item_use_flag as item_use_flag2 FROM dbo.ItemResource\"","title":"db_item/db_monster"},{"location":"documentation/daedalus-doc/#db_skilltree","text":"In the case of db_skilltree we can not read this rdb as we would the others, this is because unlike other rdb who use a single read loop based on a variable called the row count (which is part of the file header) this particular rdb uses a double read loop, where its initial loop is based off the count in the header and then a secondary count based off a field in the rdb itself. Consider the below structure example: specialCase = DOUBLELOOP fields = { { \"job_id\", INT32, flag=LOOPCOUNTER }, { \"skill_id\", INT32 }, { \"min_skill_lv\", INT32 }, { \"max_skill_lv\", INT32 }, { \"lv\", INT32 }, { \"job_lv\", INT32 }, { \"jp_ratio\", SINGLE }, { \"need_skill_id_1\", INT32 }, { \"need_skill_lv_1\", INT32 }, { \"need_skill_id_2\", INT32 }, { \"need_skill_lv_2\", INT32 }, { \"need_skill_id_3\", INT32 }, { \"need_skill_lv_3\", INT32 }, { \"cenhance_min\", INT32 }, { \"cenhance_max\", INT32 } } In this rdb structure first and foremost we need to alert the engine to the fact that this particular rdb is a special case, we will do this by defining the LUA variable specialCase In this instance our special case will be DOUBLELOOP . Once this has been defined the engine will attempt to do a secondary loop during its initial read loop, this however requires the engine know which field it will be using as the count for the secondary loop. This is where the flag variable will come into use. By defining the flag variable as LOOPCOUNTER . You are not only telling the engine to use this field during the read process but also to count the amount of rows in loaded data that have the same job_id or flag=LOOPCOUNTER so that it can write this count back down when you are saving this particular file. NOTE: at this time there is only one specialCase and it is DOUBLELOOP","title":"db_skilltree"},{"location":"documentation/daedalus-doc/#using-lua-processrow","text":"In the case of db_monster we finally have to do some row processing with our trusty ProcessRow function, in the case of this particular rdb the id field has had it's bits scrambled when it was written. This means if you just load the rdb as normal you will not get the proper id. In order to counteract this we will need to perform some bitwise operations on this field for each row as they are loaded. For such cases Daedalus already has a built in feature. Simply define the ProcessRow function. Consider the below example: local encodeMap = {} local decodeMap = {} function compute_bit_swapping() local j, oldValue for i = 0,31 do encodeMap[i] = i; end j = 3 for i = 0,31 do oldValue = encodeMap[i] encodeMap[i] = encodeMap[j] encodeMap[j] = oldValue j = (j + i + 3) % 32 end for i = 0,31 do decodeMap[encodeMap[i]] = i end end compute_bit_swapping() ProcessRow = function (mode, row, rowNum) local value = row[\"id\"] local result = 0 if mode == READ then for i = 0,31 do if bit32.band(bit32.lshift(1, i), value) ~= 0 then result = bit32.bor(result, (bit32.lshift(1, decodeMap[i]))) end end elseif mode == WRITE then for i = 0,31 do if bit32.band(bit32.lshift(1, i), value) ~= 0 then result = bit32.bor(result, (bit32.lshift(1, encodeMap[i]))) end end end row[\"id\"] = result end Notes: The ProcessRow function must always present the following arguments (mode, row, rowNum) Any function call like compute_bit_swapping() will occur automatically and doesn't need to be called manually within ProcessRow If ProcessRow has been defined it will be called once per row read / write operation In this above example we provide the encode/decode maps by defining the tables and computing the values they will hold by calling compute_bit_swapping() so when ProcessRow is called by the engine it will already have access to these variables. Now you can alter the current row being processed by Daedalus because it was passed into the ProcessRow function as the row argument. In this case we need to perform the bitwise operation on the field id so we define the local value as row[\"id\"] , here you can use strongly typed names as the key or you can provide the oridinal value which in this case would be row[0]. Then at the end of this operation we set the row[\"id\"] value to the newly decoded value and now we have the proper id when displayed.","title":"Using LUA ProcessRow"},{"location":"documentation/daedalus-doc/#sql","text":"The last topic we will be covering is the proper usage of the LUA variables sqlColumns and selectStatement These two variables should only ever be defined in the case that your provided structure (e.g. fields) can not result in a proper insert or select statement. For instance if you are selecting data which needs to be combined from two tables in the instance of db_skill or inserting data from a rdb whose epic or structure does not match the epic or structure of the target table. Example sqlColumns for db_npcresource (for 7.3): -- To send rdb data to a database table of different epic you will need to define -- the sql table structure below, in this instance the data is going backwards to 7.2 -- table. If you are sending information between matching epics, do not define sqlColumns sqlColumns = { \"id\", \"text_id\", \"name_text_id\", \"race_id\", \"sexsual_id\", \"x\", \"y\", \"z\", \"face\", \"local_flag\", \"is_periodic\", \"begin_of_period\", \"end_of_period\", \"face_x\", \"face_y\", \"face_z\", \"model_file\", \"hair_id\", \"face_id\", \"body_id\", \"weapon_item_id\", \"shield_item_id\", \"clothes_item_id\", \"helm_item_id\", \"gloves_item_id\", \"boots_item_id\", \"belt_item_id\", \"mantle_item_id\", \"necklace_item_id\", \"earring_item_id\", \"ring1_item_id\", \"ring2_item_id\", \"motion_id\", \"is_roam\", \"roaming_id\", \"standard_walk_speed\", \"standard_run_speed\", \"walk_speed\", \"run_speed\", \"attackable\", \"offensive_type\", \"spawn_type\", \"chase_range\", \"regen_time\", \"level\", \"stat_id\", \"attack_range\", \"attack_speed_type\", \"hp\", \"mp\", \"attack_point\", \"magic_point\", \"defence\", \"magic_defence\", \"attack_speed\", \"magic_speed\", \"accuracy\", \"avoid\", \"magic_accuracy\", \"magic_avoid\", \"ai_script\", \"contact_script\", \"texture_group\", --\"type\" <this field does not exist in 7.2> } Example select for db_skill: -- Use DISCTINT keyword to avoid duplicates left by gala selectStatement = \"SELECT DISTINCT [id],[text_id],[is_valid],[elemental],[is_passive],[is_physical_act],[is_harmful],[is_need_target],[is_corpse],[is_toggle],[toggle_group],[casting_type],[casting_level],[cast_range],[valid_range],[cost_hp],[cost_hp_per_skl],[cost_mp],[cost_mp_per_skl],[cost_mp_per_enhance],[cost_hp_per],[cost_hp_per_skl_per],[cost_mp_per],[cost_mp_per_skl_per],[cost_havoc],[cost_havoc_per_skl],[cost_energy],[cost_energy_per_skl],[cost_exp],[cost_exp_per_enhance],[cost_jp],[cost_jp_per_enhance],[cost_item],[cost_item_count],[cost_item_count_per_skl],[need_level],[need_hp],[need_mp],[need_havoc],[need_havoc_burst],[need_state_id],[need_state_level],[need_state_exhaust],[vf_one_hand_sword],[vf_two_hand_sword],[vf_double_sword],[vf_dagger],[vf_double_dagger],[vf_spear],[vf_axe],[vf_one_hand_axe],[vf_double_axe],[vf_one_hand_mace],[vf_two_hand_mace],[vf_lightbow],[vf_heavybow],[vf_crossbow],[vf_one_hand_staff],[vf_two_hand_staff],[vf_shield_only],[vf_is_not_need_weapon],[delay_cast],[delay_cast_per_skl],[delay_cast_mode_per_enhance],[delay_common],[delay_cooltime],[delay_cooltime_mode_per_enhance],[cool_time_group_id],[uf_self],[uf_party],[uf_guild],[uf_neutral],[uf_purple],[uf_enemy],[tf_avatar],[tf_summon],[tf_monster],[skill_lvup_limit],[target],[effect_type],[state_id],[state_level_base],[state_level_per_skl],[state_level_per_enhance],[state_second],[state_second_per_level],[state_second_per_enhance],[state_type],[probability_on_hit],[probability_inc_by_slv],[hit_bonus],[hit_bonus_per_enhance],[percentage],[hate_mod],[hate_basic],[hate_per_skl],[hate_per_enhance],[critical_bonus],[critical_bonus_per_skl],[var1],[var2],[var3],[var4],[var5],[var6],[var7],[var8],[var9],[var10],[var11],[var12],[var13],[var14],[var15],[var16],[var17],[var18],[var19],[var20],[jp_01],[jp_02],[jp_03],[jp_04],[jp_05],[jp_06],[jp_07],[jp_08],[jp_09],[jp_10],[jp_11],[jp_12],[jp_13],[jp_14],[jp_15],[jp_16],[jp_17],[jp_18],[jp_19],[jp_20],[jp_21],[jp_22],[jp_23],[jp_24],[jp_25],[jp_26],[jp_27],[jp_28],[jp_29],[jp_30],[jp_31],[jp_32],[jp_33],[jp_34],[jp_35],[jp_36],[jp_37],[jp_38],[jp_39],[jp_40],[jp_41],[jp_42],[jp_43],[jp_44],[jp_45],[jp_46],[jp_47],[jp_48],[jp_49],[jp_50],[desc_id],[tooltip_id],[icon_id],[icon_file_name],[is_projectile],[projectile_speed],[projectile_acceleration] FROM dbo.SkillResource s LEFT JOIN dbo.SkillJpResource sj ON sj.skill_id = s.id\"","title":"SQL"},{"location":"documentation/datacore-doc/","text":"Description Provides interactibility with the Rappelz proprietary Data.xxx file storage system. Namespaces DataCore DataCore.Functions DataCore.Structures Important Classes IndexEntry The IndexEntry stores information regarding a file stored in the data.xxx Properties Name Type Description Name string The file name (including extension) of this entry Extension string The file extension of this entry (does not include preceeding . ) Hash byte[] The unhashed file name of this entry HashName string The hashed file name of this entry Length int The length of this entry Offset int64 The location of this entry in the housing data.xxx DataID int The data file this entry is stored in (e.g. data.001 ) DataPath string The folder this entry would be stored at when using the vanilla PatchServer Constructors Arguments Description none Dummy/Generic constructor bool backup, Encoding encoding Instantiate the core with only backup toggled and encoding set. bool backup, string configPath Not implemented Encoding encoding, bool backup, string configPath Not Implemented Events Name Arguments Description CurrentMaxDetermined int Maximum Informs the calling application of the value to be used as Progress Maximum CurrentProgressChanged int Value Informs the calling application that the progress of the current operation has changed. CurrentProgressReset bool WriteOK Informs the calling application that the current operation has completed and the progressbar should be reset. (In the case of console apps it also determines if an [OK] and line break should be appended to the current line) WarningOccured string warning Informs the calling application of a non-critical error that has occured. MessageOccured string message Informs the calling application of a message that should be displayed to the user. (In the case of console apps it also includes formatting such as tabs and line breaks) Properties Name Type Description DataDirectory string The directory containing data.xxx files. Index List<IndexEntry> Stores all file entries indexed by the data.000 RowCount int The count of loaded IndexEntry entries. ExtensionsList List<ExtensionInfo> Collection of extensions present in the loaded Index Methods Name Arguments Return Description Load string path (of data.000) void Loads a data.000 at the given path Save string path void Saves the contents of Index to to the given path Sort SortType type void Sorts a previously loaded data.000 index EntryExists string name bool Determines if an IndexEntry exists in the loaded data.000 GetStoredSize List<IndexEntry> filteredList int Calculates the total size of the files in the filteredList FindAll string fieldName, string op, object criteria List<IndexEntry> Locates all IndexEntry with matching criteria and returns them as a list. GetEntry int index IndexEntry Returns the IndexEntry at the given index GetEntry string name IndexEntry Returns the IndexEntry with matching name GetEntry int dataID, int offset IndexEntry Returns the IndexEntry with matching dataID and offset GetEntriesByPartialName string partialName List<IndexEntry> Returns a list of all entries whose names contains partialName (You can use wildcards in the partialName) (Update by Gangor) GetEntriesByDataId int dataId, SortType type List<IndexEntry> Returns a list of all entries with matching dataId , sortable by type . GetEntriesByDataId List<IndexEntry> filteredIndex, int dataId, SortType type List<IndexEntry> Returns a list of all entries with matching dataId and type from the filteredIndex GetEntriesByExtension string extension List<IndexEntry> Returns a list of all entries with matching extension GetEntriesByExtension string extension, SortType type List<IndexEntry> Returns a list of all entries with matching extension sorted by type GetEntriesByExtension string extension, string term List<IndexEntry> Returns a list of all entries whose name contains term and extension matches extension (Overload by Gangor) GetEntriesByExtension string extension, string term, SortType type List<IndexEntry> Returns a list of all entries whose name contains term and extension matches extension sorted on type GetEntriesByExtension string extension, int dataId List<IndexEntry> Returns a list of all entries whose extension matches extension and dataId matches dataId GetEntriesByExtensions string[] extensions List<IndexEntry> Returns a list of all entries whose extension match extensions DeleteEntriesByDataId int dataId void Removes all entries with matching dataId from the loaded index DeleteEntryByName string fileName, string dataDirectory void Deletes an entry and it's stored file entry from the loaded index DeleteEntryByIdAndOffset int id, int offset void Deletes an entry based on a provided id and offset GetFileBytes string fileName byte[] Returns the collection of bytes that represent the given fileName GetFileBytes string fileName, int dataId, long offset, long length byte[] Returns the collection of bytes that represent the file in the given data.xx {dataId} at offset with length GetFileBytes IndexEntry entry byte[] Returns the collection of bytes that represent the file described by entry ExportFileEntry string buildPath , IndexEntry entry void Exports the file associated to the given entry ExportExtEntries string buildDirectory, string extension void Exports all files whose extension match the provided extension ExportAllEntries string buildDirectory void Exports all files ImportFileEntry string filePath void Adds the file at filePath to the Rappelz data.xxx file storage system ImportFileEntry string fileName, byte[] fileBytes void Adds the file represented by fileBytes to the Rappelz data.xxx file storage system as name fileName (Suggested by Gangor) BuildDataFiles string dumpDirectory, string buildPath List<IndexEntry> Builds new data.001-008 from the provided dump directory and returns the associated data.000 index RebuildDataFile int dataId, string buildDirectory void Rebuilds the data.00{ dataId } into the buildDirectory Examples To add DataCore functionality to your .net class add the using directive. using DataCore; Loading data.000 using System.Text; using DataCore; class Program() { static void Main(string[] args) { //Encoding is optional, but suggested! Encoding encoding = Encoding.Default; //Backup is optional, but suggested! bool backups = true; string path = \"C:/Rappelz/Client/data.000\"; Core core = new Core(backups, encoding); core.Load(path); Console.WriteLine($\"{core.RowCount} entries loaded from {path}\"); } } Fetching IndexEntry using System.Text; using DataCore; using DataCore.Structures; class Program() { static void Main(string[] args) { //Encoding is optional, but suggested! Encoding encoding = Encoding.Default; //Backup is optional, but suggested! bool backups = true; string path = \"C:/Rappelz/Client/data.000\"; Core core = new Core(backups, encoding); core.Load(path); Console.WriteLine($\"{core.RowCount} entries loaded from {path}\"); IndexEntry entry = core.GetEntry(\"db_string.rdb\"); if (entry != null) Console.WriteLine(\"db_string fetched successfully!\"); } } Exporting File'(s) Single using System.Text; using DataCore; using DataCore.Structures; class Program() { static void Main(string[] args) { //Encoding is optional, but suggested! Encoding encoding = Encoding.Default; //Backup is optional, but suggested! bool backups = true; string path = \"C://Rappelz//Client//data.000\"; Core core = new Core(backups, encoding); core.Load(path); Console.WriteLine($\"{core.RowCount} entries loaded from {path}\"); IndexEntry entry = core.GetEntry(\"db_string.rdb\"); if (entry != null) { string buildDir = \"c://Rappelz//Output\" core.ExportFileEntry(buildDir, entry); } } } Multiple using System.Text; using DataCore; using DataCore.Structures; class Program() { static void Main(string[] args) { //Encoding is optional, but suggested! Encoding encoding = Encoding.Default; //Backup is optional, but suggested! bool backups = true; string path = \"C://Rappelz//Client//data.000\"; Core core = new Core(backups, encoding); core.Load(path); Console.WriteLine($\"{core.RowCount} entries loaded from {path}\"); string ext = \"jpg\"; List<IndexEntry> entries = core.GetEntriesByExtension(ext); if (entries.Count > 0) { string buildDir = \"c://Rappelz//Output\" core.ExportExtEntries(buildDir, entries); } } } Special Thanks xXExiledXx Glandu2 Gangor","title":"DataCore Documentation"},{"location":"documentation/datacore-doc/#description","text":"Provides interactibility with the Rappelz proprietary Data.xxx file storage system.","title":"Description"},{"location":"documentation/datacore-doc/#namespaces","text":"DataCore DataCore.Functions DataCore.Structures","title":"Namespaces"},{"location":"documentation/datacore-doc/#important-classes","text":"","title":"Important Classes"},{"location":"documentation/datacore-doc/#indexentry","text":"The IndexEntry stores information regarding a file stored in the data.xxx","title":"IndexEntry"},{"location":"documentation/datacore-doc/#properties","text":"Name Type Description Name string The file name (including extension) of this entry Extension string The file extension of this entry (does not include preceeding . ) Hash byte[] The unhashed file name of this entry HashName string The hashed file name of this entry Length int The length of this entry Offset int64 The location of this entry in the housing data.xxx DataID int The data file this entry is stored in (e.g. data.001 ) DataPath string The folder this entry would be stored at when using the vanilla PatchServer","title":"Properties"},{"location":"documentation/datacore-doc/#constructors","text":"Arguments Description none Dummy/Generic constructor bool backup, Encoding encoding Instantiate the core with only backup toggled and encoding set. bool backup, string configPath Not implemented Encoding encoding, bool backup, string configPath Not Implemented","title":"Constructors"},{"location":"documentation/datacore-doc/#events","text":"Name Arguments Description CurrentMaxDetermined int Maximum Informs the calling application of the value to be used as Progress Maximum CurrentProgressChanged int Value Informs the calling application that the progress of the current operation has changed. CurrentProgressReset bool WriteOK Informs the calling application that the current operation has completed and the progressbar should be reset. (In the case of console apps it also determines if an [OK] and line break should be appended to the current line) WarningOccured string warning Informs the calling application of a non-critical error that has occured. MessageOccured string message Informs the calling application of a message that should be displayed to the user. (In the case of console apps it also includes formatting such as tabs and line breaks)","title":"Events"},{"location":"documentation/datacore-doc/#properties_1","text":"Name Type Description DataDirectory string The directory containing data.xxx files. Index List<IndexEntry> Stores all file entries indexed by the data.000 RowCount int The count of loaded IndexEntry entries. ExtensionsList List<ExtensionInfo> Collection of extensions present in the loaded Index","title":"Properties"},{"location":"documentation/datacore-doc/#methods","text":"Name Arguments Return Description Load string path (of data.000) void Loads a data.000 at the given path Save string path void Saves the contents of Index to to the given path Sort SortType type void Sorts a previously loaded data.000 index EntryExists string name bool Determines if an IndexEntry exists in the loaded data.000 GetStoredSize List<IndexEntry> filteredList int Calculates the total size of the files in the filteredList FindAll string fieldName, string op, object criteria List<IndexEntry> Locates all IndexEntry with matching criteria and returns them as a list. GetEntry int index IndexEntry Returns the IndexEntry at the given index GetEntry string name IndexEntry Returns the IndexEntry with matching name GetEntry int dataID, int offset IndexEntry Returns the IndexEntry with matching dataID and offset GetEntriesByPartialName string partialName List<IndexEntry> Returns a list of all entries whose names contains partialName (You can use wildcards in the partialName) (Update by Gangor) GetEntriesByDataId int dataId, SortType type List<IndexEntry> Returns a list of all entries with matching dataId , sortable by type . GetEntriesByDataId List<IndexEntry> filteredIndex, int dataId, SortType type List<IndexEntry> Returns a list of all entries with matching dataId and type from the filteredIndex GetEntriesByExtension string extension List<IndexEntry> Returns a list of all entries with matching extension GetEntriesByExtension string extension, SortType type List<IndexEntry> Returns a list of all entries with matching extension sorted by type GetEntriesByExtension string extension, string term List<IndexEntry> Returns a list of all entries whose name contains term and extension matches extension (Overload by Gangor) GetEntriesByExtension string extension, string term, SortType type List<IndexEntry> Returns a list of all entries whose name contains term and extension matches extension sorted on type GetEntriesByExtension string extension, int dataId List<IndexEntry> Returns a list of all entries whose extension matches extension and dataId matches dataId GetEntriesByExtensions string[] extensions List<IndexEntry> Returns a list of all entries whose extension match extensions DeleteEntriesByDataId int dataId void Removes all entries with matching dataId from the loaded index DeleteEntryByName string fileName, string dataDirectory void Deletes an entry and it's stored file entry from the loaded index DeleteEntryByIdAndOffset int id, int offset void Deletes an entry based on a provided id and offset GetFileBytes string fileName byte[] Returns the collection of bytes that represent the given fileName GetFileBytes string fileName, int dataId, long offset, long length byte[] Returns the collection of bytes that represent the file in the given data.xx {dataId} at offset with length GetFileBytes IndexEntry entry byte[] Returns the collection of bytes that represent the file described by entry ExportFileEntry string buildPath , IndexEntry entry void Exports the file associated to the given entry ExportExtEntries string buildDirectory, string extension void Exports all files whose extension match the provided extension ExportAllEntries string buildDirectory void Exports all files ImportFileEntry string filePath void Adds the file at filePath to the Rappelz data.xxx file storage system ImportFileEntry string fileName, byte[] fileBytes void Adds the file represented by fileBytes to the Rappelz data.xxx file storage system as name fileName (Suggested by Gangor) BuildDataFiles string dumpDirectory, string buildPath List<IndexEntry> Builds new data.001-008 from the provided dump directory and returns the associated data.000 index RebuildDataFile int dataId, string buildDirectory void Rebuilds the data.00{ dataId } into the buildDirectory","title":"Methods"},{"location":"documentation/datacore-doc/#examples","text":"To add DataCore functionality to your .net class add the using directive. using DataCore;","title":"Examples"},{"location":"documentation/datacore-doc/#loading-data000","text":"using System.Text; using DataCore; class Program() { static void Main(string[] args) { //Encoding is optional, but suggested! Encoding encoding = Encoding.Default; //Backup is optional, but suggested! bool backups = true; string path = \"C:/Rappelz/Client/data.000\"; Core core = new Core(backups, encoding); core.Load(path); Console.WriteLine($\"{core.RowCount} entries loaded from {path}\"); } }","title":"Loading data.000"},{"location":"documentation/datacore-doc/#fetching-indexentry","text":"using System.Text; using DataCore; using DataCore.Structures; class Program() { static void Main(string[] args) { //Encoding is optional, but suggested! Encoding encoding = Encoding.Default; //Backup is optional, but suggested! bool backups = true; string path = \"C:/Rappelz/Client/data.000\"; Core core = new Core(backups, encoding); core.Load(path); Console.WriteLine($\"{core.RowCount} entries loaded from {path}\"); IndexEntry entry = core.GetEntry(\"db_string.rdb\"); if (entry != null) Console.WriteLine(\"db_string fetched successfully!\"); } }","title":"Fetching IndexEntry"},{"location":"documentation/datacore-doc/#exporting-files","text":"","title":"Exporting File'(s)"},{"location":"documentation/datacore-doc/#single","text":"using System.Text; using DataCore; using DataCore.Structures; class Program() { static void Main(string[] args) { //Encoding is optional, but suggested! Encoding encoding = Encoding.Default; //Backup is optional, but suggested! bool backups = true; string path = \"C://Rappelz//Client//data.000\"; Core core = new Core(backups, encoding); core.Load(path); Console.WriteLine($\"{core.RowCount} entries loaded from {path}\"); IndexEntry entry = core.GetEntry(\"db_string.rdb\"); if (entry != null) { string buildDir = \"c://Rappelz//Output\" core.ExportFileEntry(buildDir, entry); } } }","title":"Single"},{"location":"documentation/datacore-doc/#multiple","text":"using System.Text; using DataCore; using DataCore.Structures; class Program() { static void Main(string[] args) { //Encoding is optional, but suggested! Encoding encoding = Encoding.Default; //Backup is optional, but suggested! bool backups = true; string path = \"C://Rappelz//Client//data.000\"; Core core = new Core(backups, encoding); core.Load(path); Console.WriteLine($\"{core.RowCount} entries loaded from {path}\"); string ext = \"jpg\"; List<IndexEntry> entries = core.GetEntriesByExtension(ext); if (entries.Count > 0) { string buildDir = \"c://Rappelz//Output\" core.ExportExtEntries(buildDir, entries); } } }","title":"Multiple"},{"location":"documentation/datacore-doc/#special-thanks","text":"xXExiledXx Glandu2 Gangor","title":"Special Thanks"},{"location":"documentation/grimoire-doc/","text":"Grimoire Documentation First Use It is recommended that users configure Grimoire when running it for the first time, this can be done via opening Config.json in your preferred editor or by clicking the Settings button in the tool itself. Below you will find a handy reference of the settings. Settings Name Category Description Value-Type BuildDirectory Grim Directory where files created by Grimoire will be written to string DumpDirectory Grim Directory where utilities like SPR Generator and Dump Updater can find a premade /dump/ string Codepage Grim Encoding used in some functionality int Styles Tab Styles to be listed in the New: tab list string[] DefaultStyle Tab Style to be loaded on-start (if any, elsewise use \"NONE\") string Engine DB Type of database engine to be used (0 = MsSQL, 1 = MySQL) int IP DB Target machine to connect to for SQL operations. string Port DB Target machine port to connections related to SQL operations. int Trusted DB If true Grimoire will attempt to make trusted connections on the SA account for MsSQL connections. Note: this will only work when your SQL instance is installed as a default instance with Mixed Mode Authentication enabled on your LOCAL network. bool WorldName DB Name of the Arcadia database. string WorldUser DB Username for the Arcadia database. (Leave blank if using Trusted) string WorldPass DB Password for the user of the Arcadia database. (Leave blank if using Trusted) string Backup DB If true, tables will be backed up via scripting before changes are made to them bool DropOnExport DB If true, tables will be dropped via the drop statement during save operations. bool Timeout DB The amount of time that can elapse before SQL operations will timeout. int Struct_AutoLoad RDB Not implemented! bool Directory RDB Directory where structures can be located. (/structures by default) bool Encoding RDB Encoding to be used in rdb related operations. Note: this must match a name on the list of encoding for RDB tabs. string LoadDirectory RDB Default directory to be used in .rdb load/save operations. string AppendASCII RDB If true, (ascii) will be appended to the names of files being written to disk. bool SaveHashed RDB If true, file names will be converted/hashed during save operations. bool CSV_Directory RDB Directory where .csv will be created. string SQL_Directory RDB Not implemented. string LoadDirectory Data Default directory be used in data.000 load/save operations. string Encoding Data Encoding used in data.xxx operations int Backup Data If true, data.xxx files will be backed up before being changed by Grimoire bool UseModifiedXOR Data If true, DataCore will use the provided ModifiedXORKey in operations bool ModifiedXORKey Data Key to be used in DataCore operations if UseModifiedXOR is set to true int[] (array) ClearOnCreate Data If true, newly created data.000 will not be loaded and displayed bool AutoClear Hash If true, the hash grid will be cleared after a conversion occurs. bool AutoConvert Hash If true, files added to the hash grid will be automatically hashed. bool Type Hash Do not change this! int Directory Log Directory where new log files will be written string DisplayLevel Log Not Implemented! int WriteLevel Log Not Implemented! int RefreshInterval Log Time (in seconds) before the log manager will refresh the displayed logged entries. int SaveOnExit Log If true, logs will be written to disk during the exit operation. bool Directory Flag Directory where flag list .txt are loaded from string Default Flag Name (without extension) of the default flag list .txt string ClearOnChange Flag If true, when flag lists are changed all selected flags will be unselected. bool OverwriteExisting DumpUpdater If true, files in the target dump directory will be automatically ovewritten during an update operation bool IgnoreSize SPR If true, the spr generator will not warn/delete of icons with a size different than expected bool ShowWarnings SPR If true, the spr generator will show warnings about icons that could not be validated and have been removed bool ShowIgnored SPR If true, the spr generator will show a list of all ignored (unvalidated) icon files after a generate operation bool Directory Localization Directory where localization .xml are located string Locale Localization Name (without extension) of the target locale .xml string Using Data Utility Most of the instructions below assume you have an open and properly configured instance of Grimoire Opening the Rappelz file index (data.000) The data.000 can be opened in a couple ways: Click the Load button In Windows Explorer locate the desired data.000 and drag it on-top of the data tab. Press the CTRL + O key combination Creating new data.xxx (Please note new data files will be created in the output folder, if backups is toggled than any existing data.xxx in this directory will be preserved, otherwise they will be overwritten) Open a new Data tab Click New or press the CTRL + N key combination Select the folder containing a dump The folder should contain files categorized by the extension e.g. cob/dds/jpg etc Any files out of place will cause File Not Found exceptions. (e.g. a .nfe file located in the /nfm/ directory!) Wait til complete! Rebuild / Shrink client data.xxx Open new Data tab Open a data.000 index Click Rebuild or press the CTRL + R key combination Follow steps of the rebuild wizard Exporting Files Right click All in the extensions list and click Export or Locate the specific extension you want to export and right click it and click Export or Search for or otherwise scroll to and select (you may select more than 1) then right click after selecting desired files and click Export Comparing files Search for or scroll to the desired file Right click the file grid and click Compare !!! It should be noted, this currently will not function for rdb due to the header date changing! !!! Inserting/Deleting files Search for or scroll to the desired file (for delete) Right click the grid and select Insert/Delete Follow prompts You can also simply drag new files (to be inserted) directly onto a loaded data index Using RDB Utility Most of the instructions below assume you have an open and properly configured instance of Grimoire Opening RDB RDB data can be loaded in multiple ways: Click the Load button Click the File button and select the desired .rdb or Locate the .rdb in in Windows Explorer Drag it onto a clean .rdb tab or Press CTRL + O to open File Dialogue Select the desired .rdb or Locate client directory containing desired .rdb Load the index (data.000) in the same way a normal rdb would be loaded Select the desired RDB from the list of selectable rdb (contained in this client) or Click the Load button Click the SQL button Saving RDB Before saving you should make sure you have (ascii) / encrypted (buttons on the toolbar of the rdb tab) are set properly. Click the Save button Hover mouse over File Select appropriate type to save as. or Press the CTRL + S key combination Searching To find a specific value in a specific field, simply press CTRL + F . Select the desired field and enter the desired value to be matched, then press OK. If the value exists, it will be automatically selected. Sorting Click the column header to sort, clicking it again will reverse the sort order. Editing Bit Flags Grimoire has the ability to launch the Bit Flag Utility from within an rdb tab instance, to enable this simply see the example given in the ItemResource72.lua (structure) {\"item_use_flag\", INT32, flag=BIT_FLAG, opt={ \"use_flags_under_81\" }}, The bit flag utility accepts any 4 byte integer as a value, so if you have a field such as the \"item_use_flag\" who has the fieldtype \"INT32\" you can add the flag \"BIT_FLAG\" to it, this specifies that when you right click on this field in a loaded RDB tab, you can then click \"Open w/ Flag Editor\" You can go further than this by specifying the opt variable and giving it the name of the default flag list (.txt) we want to be loaded to process the value in the current cell. Using Hash Utility You should make sure you have properly configured the Hash Utility related settings Single Name (Real Time) Hashing Locate the 'Instant' group of controls. (Upper left hand corner of the tab) Enter a plain-text or hashed name The opposite will be generated in the 'Output' textbox. (You can also use the flip button to flip the results) Multiple File Hashing You can load files to be converted into the Hash Utility in multiple ways: Locate the file'(s) in Windows Explorer Select the file'(s) and drag them onto the 'Multi-File' grid (If 'Auto-Convert' is not enabled) Right click the grid and hover over 'Convert' Click All or Locate the folder containing files to be hashed in Windows Explorerer Drag the folder onto the Multi-File grid (If 'Auto-Convert' is not enabled) Right click the grid and hover over Convert Click All Add/Remove (ascii) During the name conversion (hashing) process, you can append (ascii) to all files being convert by simply enabling the 'Append (ascii)' radio button in the 'Name Options' groupbox. Using Bit Flag Editor Open the Bit Flag Editor from the Utilities menu Select the desired flag list 3 Modify flag selection as desired Copy output Paste flag into the Input/Output text box to decode Using Dump Updater (Make sure that you have configured settings related to this utility before use!) Open the Dump Updater from the Utilities menu Verify the Dump Directory is the desired directory Drag-N-Drop Folder (or files) onto the utility Check the listed files and remove any unwanted/accidental additions Click Copy then confirm you want to start the process! Using SPR Generator Open the SPR Generator from the Utilities menu Select the type of SPR you want to generate Click Generate (The utility will compare item/skill/state icon listings in the database to the dump directory listed in your settings to determine if a matching icon actually exists. If a matching icon is not found the icon will not be listed in the generated spr!) Using XOR Editor Open the XOR Editor from the Utilities menu Load XOR Key You can load the Default key, a key stored in a key file or the key stored in your Config.json if applicable Edit the key as desired The unResourceEncodeKey is updated in real time If generating a new key for the sframe, you can right click the hex editor and click Copy and simply paste the copied output into the XORen.cpp::szResourceEncodeKey (do the same for the unResourceEncodeKey ) Save the key You can save the key directly to the Config.json as-well as a key file (which can be shared with developers on your team) Generated key files do not automatically save into the Config.json !!! (This goes for loading them as-well!) Using Modified XOR Key If you have modified the szResourceEncodeKey / unResourceEncodeKey of your SFrame.exe XORen.cpp you will need to provide this modified key to Grimoire in order to create and load custom data.xxx files.","title":"Grimoire Documentation"},{"location":"documentation/grimoire-doc/#grimoire-documentation","text":"","title":"Grimoire Documentation"},{"location":"documentation/grimoire-doc/#first-use","text":"It is recommended that users configure Grimoire when running it for the first time, this can be done via opening Config.json in your preferred editor or by clicking the Settings button in the tool itself. Below you will find a handy reference of the settings.","title":"First Use"},{"location":"documentation/grimoire-doc/#settings","text":"Name Category Description Value-Type BuildDirectory Grim Directory where files created by Grimoire will be written to string DumpDirectory Grim Directory where utilities like SPR Generator and Dump Updater can find a premade /dump/ string Codepage Grim Encoding used in some functionality int Styles Tab Styles to be listed in the New: tab list string[] DefaultStyle Tab Style to be loaded on-start (if any, elsewise use \"NONE\") string Engine DB Type of database engine to be used (0 = MsSQL, 1 = MySQL) int IP DB Target machine to connect to for SQL operations. string Port DB Target machine port to connections related to SQL operations. int Trusted DB If true Grimoire will attempt to make trusted connections on the SA account for MsSQL connections. Note: this will only work when your SQL instance is installed as a default instance with Mixed Mode Authentication enabled on your LOCAL network. bool WorldName DB Name of the Arcadia database. string WorldUser DB Username for the Arcadia database. (Leave blank if using Trusted) string WorldPass DB Password for the user of the Arcadia database. (Leave blank if using Trusted) string Backup DB If true, tables will be backed up via scripting before changes are made to them bool DropOnExport DB If true, tables will be dropped via the drop statement during save operations. bool Timeout DB The amount of time that can elapse before SQL operations will timeout. int Struct_AutoLoad RDB Not implemented! bool Directory RDB Directory where structures can be located. (/structures by default) bool Encoding RDB Encoding to be used in rdb related operations. Note: this must match a name on the list of encoding for RDB tabs. string LoadDirectory RDB Default directory to be used in .rdb load/save operations. string AppendASCII RDB If true, (ascii) will be appended to the names of files being written to disk. bool SaveHashed RDB If true, file names will be converted/hashed during save operations. bool CSV_Directory RDB Directory where .csv will be created. string SQL_Directory RDB Not implemented. string LoadDirectory Data Default directory be used in data.000 load/save operations. string Encoding Data Encoding used in data.xxx operations int Backup Data If true, data.xxx files will be backed up before being changed by Grimoire bool UseModifiedXOR Data If true, DataCore will use the provided ModifiedXORKey in operations bool ModifiedXORKey Data Key to be used in DataCore operations if UseModifiedXOR is set to true int[] (array) ClearOnCreate Data If true, newly created data.000 will not be loaded and displayed bool AutoClear Hash If true, the hash grid will be cleared after a conversion occurs. bool AutoConvert Hash If true, files added to the hash grid will be automatically hashed. bool Type Hash Do not change this! int Directory Log Directory where new log files will be written string DisplayLevel Log Not Implemented! int WriteLevel Log Not Implemented! int RefreshInterval Log Time (in seconds) before the log manager will refresh the displayed logged entries. int SaveOnExit Log If true, logs will be written to disk during the exit operation. bool Directory Flag Directory where flag list .txt are loaded from string Default Flag Name (without extension) of the default flag list .txt string ClearOnChange Flag If true, when flag lists are changed all selected flags will be unselected. bool OverwriteExisting DumpUpdater If true, files in the target dump directory will be automatically ovewritten during an update operation bool IgnoreSize SPR If true, the spr generator will not warn/delete of icons with a size different than expected bool ShowWarnings SPR If true, the spr generator will show warnings about icons that could not be validated and have been removed bool ShowIgnored SPR If true, the spr generator will show a list of all ignored (unvalidated) icon files after a generate operation bool Directory Localization Directory where localization .xml are located string Locale Localization Name (without extension) of the target locale .xml string","title":"Settings"},{"location":"documentation/grimoire-doc/#using-data-utility","text":"Most of the instructions below assume you have an open and properly configured instance of Grimoire","title":"Using Data Utility"},{"location":"documentation/grimoire-doc/#opening-the-rappelz-file-index-data000","text":"The data.000 can be opened in a couple ways: Click the Load button In Windows Explorer locate the desired data.000 and drag it on-top of the data tab. Press the CTRL + O key combination","title":"Opening the Rappelz file index (data.000)"},{"location":"documentation/grimoire-doc/#creating-new-dataxxx","text":"(Please note new data files will be created in the output folder, if backups is toggled than any existing data.xxx in this directory will be preserved, otherwise they will be overwritten) Open a new Data tab Click New or press the CTRL + N key combination Select the folder containing a dump The folder should contain files categorized by the extension e.g. cob/dds/jpg etc Any files out of place will cause File Not Found exceptions. (e.g. a .nfe file located in the /nfm/ directory!) Wait til complete!","title":"Creating new data.xxx"},{"location":"documentation/grimoire-doc/#rebuild-shrink-client-dataxxx","text":"Open new Data tab Open a data.000 index Click Rebuild or press the CTRL + R key combination Follow steps of the rebuild wizard","title":"Rebuild / Shrink client data.xxx"},{"location":"documentation/grimoire-doc/#exporting-files","text":"Right click All in the extensions list and click Export or Locate the specific extension you want to export and right click it and click Export or Search for or otherwise scroll to and select (you may select more than 1) then right click after selecting desired files and click Export","title":"Exporting Files"},{"location":"documentation/grimoire-doc/#comparing-files","text":"Search for or scroll to the desired file Right click the file grid and click Compare !!! It should be noted, this currently will not function for rdb due to the header date changing! !!!","title":"Comparing files"},{"location":"documentation/grimoire-doc/#insertingdeleting-files","text":"Search for or scroll to the desired file (for delete) Right click the grid and select Insert/Delete Follow prompts You can also simply drag new files (to be inserted) directly onto a loaded data index","title":"Inserting/Deleting files"},{"location":"documentation/grimoire-doc/#using-rdb-utility","text":"Most of the instructions below assume you have an open and properly configured instance of Grimoire","title":"Using RDB Utility"},{"location":"documentation/grimoire-doc/#opening-rdb","text":"RDB data can be loaded in multiple ways: Click the Load button Click the File button and select the desired .rdb or Locate the .rdb in in Windows Explorer Drag it onto a clean .rdb tab or Press CTRL + O to open File Dialogue Select the desired .rdb or Locate client directory containing desired .rdb Load the index (data.000) in the same way a normal rdb would be loaded Select the desired RDB from the list of selectable rdb (contained in this client) or Click the Load button Click the SQL button","title":"Opening RDB"},{"location":"documentation/grimoire-doc/#saving-rdb","text":"Before saving you should make sure you have (ascii) / encrypted (buttons on the toolbar of the rdb tab) are set properly. Click the Save button Hover mouse over File Select appropriate type to save as. or Press the CTRL + S key combination","title":"Saving RDB"},{"location":"documentation/grimoire-doc/#searching","text":"To find a specific value in a specific field, simply press CTRL + F . Select the desired field and enter the desired value to be matched, then press OK. If the value exists, it will be automatically selected.","title":"Searching"},{"location":"documentation/grimoire-doc/#sorting","text":"Click the column header to sort, clicking it again will reverse the sort order.","title":"Sorting"},{"location":"documentation/grimoire-doc/#editing-bit-flags","text":"Grimoire has the ability to launch the Bit Flag Utility from within an rdb tab instance, to enable this simply see the example given in the ItemResource72.lua (structure) {\"item_use_flag\", INT32, flag=BIT_FLAG, opt={ \"use_flags_under_81\" }}, The bit flag utility accepts any 4 byte integer as a value, so if you have a field such as the \"item_use_flag\" who has the fieldtype \"INT32\" you can add the flag \"BIT_FLAG\" to it, this specifies that when you right click on this field in a loaded RDB tab, you can then click \"Open w/ Flag Editor\" You can go further than this by specifying the opt variable and giving it the name of the default flag list (.txt) we want to be loaded to process the value in the current cell.","title":"Editing Bit Flags"},{"location":"documentation/grimoire-doc/#using-hash-utility","text":"You should make sure you have properly configured the Hash Utility related settings","title":"Using Hash Utility"},{"location":"documentation/grimoire-doc/#single-name-real-time-hashing","text":"Locate the 'Instant' group of controls. (Upper left hand corner of the tab) Enter a plain-text or hashed name The opposite will be generated in the 'Output' textbox. (You can also use the flip button to flip the results)","title":"Single Name (Real Time) Hashing"},{"location":"documentation/grimoire-doc/#multiple-file-hashing","text":"You can load files to be converted into the Hash Utility in multiple ways: Locate the file'(s) in Windows Explorer Select the file'(s) and drag them onto the 'Multi-File' grid (If 'Auto-Convert' is not enabled) Right click the grid and hover over 'Convert' Click All or Locate the folder containing files to be hashed in Windows Explorerer Drag the folder onto the Multi-File grid (If 'Auto-Convert' is not enabled) Right click the grid and hover over Convert Click All","title":"Multiple File Hashing"},{"location":"documentation/grimoire-doc/#addremove-ascii","text":"During the name conversion (hashing) process, you can append (ascii) to all files being convert by simply enabling the 'Append (ascii)' radio button in the 'Name Options' groupbox.","title":"Add/Remove (ascii)"},{"location":"documentation/grimoire-doc/#using-bit-flag-editor","text":"Open the Bit Flag Editor from the Utilities menu Select the desired flag list 3 Modify flag selection as desired Copy output Paste flag into the Input/Output text box to decode","title":"Using Bit Flag Editor"},{"location":"documentation/grimoire-doc/#using-dump-updater","text":"(Make sure that you have configured settings related to this utility before use!) Open the Dump Updater from the Utilities menu Verify the Dump Directory is the desired directory Drag-N-Drop Folder (or files) onto the utility Check the listed files and remove any unwanted/accidental additions Click Copy then confirm you want to start the process!","title":"Using Dump Updater"},{"location":"documentation/grimoire-doc/#using-spr-generator","text":"Open the SPR Generator from the Utilities menu Select the type of SPR you want to generate Click Generate (The utility will compare item/skill/state icon listings in the database to the dump directory listed in your settings to determine if a matching icon actually exists. If a matching icon is not found the icon will not be listed in the generated spr!)","title":"Using SPR Generator"},{"location":"documentation/grimoire-doc/#using-xor-editor","text":"Open the XOR Editor from the Utilities menu Load XOR Key You can load the Default key, a key stored in a key file or the key stored in your Config.json if applicable Edit the key as desired The unResourceEncodeKey is updated in real time If generating a new key for the sframe, you can right click the hex editor and click Copy and simply paste the copied output into the XORen.cpp::szResourceEncodeKey (do the same for the unResourceEncodeKey ) Save the key You can save the key directly to the Config.json as-well as a key file (which can be shared with developers on your team) Generated key files do not automatically save into the Config.json !!! (This goes for loading them as-well!)","title":"Using XOR Editor"},{"location":"documentation/grimoire-doc/#using-modified-xor-key","text":"If you have modified the szResourceEncodeKey / unResourceEncodeKey of your SFrame.exe XORen.cpp you will need to provide this modified key to Grimoire in order to create and load custom data.xxx files.","title":"Using Modified XOR Key"},{"location":"documentation/mixresource-doc/","text":"Description This document will explain use cases for the dbo.MixResource table and rdb. Mix Type MIX_ENHANCE = 101, MIX_ENHANCE_SKILL_CARD = 102, MIX_ENHANCE_WITHOUT_FAIL = 103, MIX_ENHANCE_CREATURE_CARD = 104, MIX_ENHANCE_CREATURE_CARD_WITH_JOKER = 105, MIX_SET_LEVEL = 201, MIX_SET_LEVEL_CREATE_ITEM = 202, MIX_SET_LEVEL_SET_FLAG = 211, MIX_SET_LEVEL_SET_FLAG_CREATE_ITEM = 212, MIX_SET_LEVEL_SET_FLAG_CREATE_ITEM_WITH_MAIN_MATERIAL_LEVEL = 213, MIX_SET_LEVEL_ON_SUB_MATERIAL_LEVEL_SET_FLAG = 214, MIX_SET_LEVEL_SET_FLAG_CREATE_ITEM_WITH_MAIN_MATERIAL_LEVEL_SET_ZERO = 215, MIX_ADD_LEVEL = 301, MIX_ADD_LEVEL_CREATE_ITEM = 302, MIX_ADD_LEVEL_SET_FLAG = 311, MIX_ADD_LEVEL_SET_FLAG_CREATE_ITEM = 312, MIX_RECYCLE = 401, MIX_RECYCLE_ENHANCE = 402, MIX_RESTORE_ENHANCE_SET_FLAG = 501, MIX_CREATE_ITEM = 601, MIX_REPLACE_WITH = 602, MIX_SET_ELEMENTAL_EFFECT = 701, MIX_SET_ELEMENTAL_EFFECT_PARAMETER = 702, MIX_SET_SOCKET = 703, MIX_SACRIFICE_ITEM_FOR_ETHEREAL_DURABILITY_WITH_MESSAGE = 801, MIX_TRANSMIT_ETHEREAL_DURABILITY = 802, MIX_RECOVER_EXHAUSTED_ETHEREAL_DURABILITY = 803, MIX_SACRIFICE_ITEM_FOR_ETHEREAL_DURABILITY = 804, MIX_SACRIFICE_ITEM_FOR_ETHEREAL_STONE_DURABILITY = 805, MIX_TRANSMIT_ETHEREAL_DURABILITY_FROM_ETHEREAL_STONE = 806, Check Type The check enumeration is typically used in the sub0x_type_0x depending on the sub_material_count CHECK_ITEM_GROUP = 1, CHECK_ITEM_CLASS = 2, CHECK_ITEM_ID = 3, CHECK_ITEM_RANK = 4, CHECK_ITEM_LEVEL = 5, CHECK_FLAG_ON = 6, CHECK_FLAG_OFF = 7, CHECK_ENHANCE_MATCH = 8, CHECK_ENHANCE_DISMATCH = 9, CHECK_ITEM_COUNT = 10, CHECK_ELEMENTAL_EFFECT_MATCH = 11, CHECK_ELEMENTAL_EFFECT_MISMATCH = 12, CHECK_ITEM_WEAR_POSITION_MATCH = 13, CHECK_ITEM_WEAR_POSITION_MISMATCH = 14, CHECK_ITEM_COUNT_GE = 15, // The number of items exceeds the specified quantity CHECK_ITEM_ETHEREAL_DURABILITY_E = 16, // Ethereal durability is the same as the specified value CHECK_ITEM_ETHEREAL_DURABILITY_NE = 17, // Ethereal durability is different from the specified value CHECK_ITEM_GRADE = 18, // Item grade is same as specified value CHECK_SAME_ITEM_ID = 19, // Item code identical to the item in the designated slot (auxiliary material cross-reference type) CHECK_SAME_SUMMON_CODE = 20, // Minion code identical to the item in the designated slot (code material cross-reference type) CHECK_ITEM_EXPIRED_TIME_GE = 21, // The remaining time of the fixed-term item is greater than or equal to the specified value CHECK_ITEM_EXPIRED_TIME_LE = 22, // The remaining time of the fixed-term item is less than or equal to the specified value CHECK_ITEM_FIRST_SOCKET_CODE_MATCH = 23, // The code of the first slot is the same as the specified value CHECK_SAME_ITEM_ENHANCE = 24, // same enhancement as the item in the designated slot (auxiliary material cross-reference type) CHECK_SAME_SKILL_ID = 25, // The same skill ID as the item in the specified slot (auxiliary material cross-reference type) Explination by Type Below the various types of combinations will be broken down by their type (e.g. their mix_type) Create Item (601) mix_value_01 - Result item id (the item this combination will create) mix_value_02 - Result item lv (the level of the resulting item) mix_value_03 - Percent (0-100) this combination will succeeed mix_value_04 - Result item minimum count (the minimum number of the resulting item for this combination) mix_value_05 - Result item maximum count (the maximum number of the resulting item for this combination) sub_material_count - The amount of items that must be combined to create the result item. Amount of sub0*_type0* / sub0*_value_0* fields completed/used is determined by this value. sub0*_type_0* - Value enumerated by Check Type Enum sub0*_value_0* - Depends on the matching sub0*_type_0* field before it. (See following example) Example sub01_type_01: 3 sub01_value_01: 490003 sub01_type_02: 10 sub01_value_02: 1 In the above example if we refer to the Check Type Enum we can see 3 resolves to: CHECK_ITEM_ID and 10 resolves to CHECK_ITEM_COUNT . So in the above example, the first required item/ingredient for the combination is 490003 because a sub01_type_01 of 3 means we're checking the sub01_value_01 field for an item id. We are also checking to see if there is 1 of that item because the sub01_type_02 of 10 means were checking sub01_value_02 for a count. Enhance Creature Card (104/105) Note: Type 104 is enhance without Joker / 105 is enhance with Joker mix_value_01 - EnhanceResource id used in success calculation for stage +1/5 mix_value_04 - If enhance is type 104 (without joker) and the main creature card is >= +3 then this field will contain a creature card item id (likely joker 540070 ) mix_value_06 - Maximum percentage chance that the item id in mix_value_04 will be given on an enhance failure of type 104 sub_material_count - The amount of items that must be combined to create the result item. Amount of sub0*_type0* / sub0*_value_0* fields completed/used is determined by this value. main0*_type_0* - Value enumated by Check Type Enum main0*_value_0* - Depends on the matching main0*_type_0* sub0*_type_0* - Value enumerated by Check Type Enum sub0*_value_0* - Depends on the matching sub0*_type_0* field before it. (See following example) Example (type 104) type: 104 mix_value_01: 2000 mix_value_04: 540070 mix_value_06: 4 main_type_01: 1 main_value_01: 13 main_type_02: 6 main_value_02: 31 sub01_type_01: 1 sub01_value_01: 13 sub01_type_02: 6 sub01_value_02: 31 sub01_type_03: 20 sub02_type_01: 3 sub02_value_01: 710001 In the above example, we can tell we are enhancing a creature card without a joker because the type is 104 We also notice that our chance for success (per stage 1/5) can be found in the EnhanceResource table with enhance_id 2000 We also see that since we are type 104 that if our enhance fails at or above +3 (stage 3) that there is a 4% ( mix_value_06 ) chance that a Joker ( 540070 ) will be the result. By refering to Check Type Enum we know that main_type_01 (1 - CHECK_ITEM_GROUP ) means our main creature card (item) must have group 13 ( main_value_01 ) and by checking main_type_02 ( 6 - CHECK_FLAG_ON ) we can tell that the main creature card (item) must bear flag 31 ( Tamed Pet ) We can also see that sub01_type_01/02 and sub_01_value_01/02 are the same basically telling us our second creature card must be of group 13 and bearing the 31 ( Tamed Pet ) flag. Looking at sub01_type_03 ( 20 - CHECK_SAME_SUMMON_CODE ) we can see that both the main and combined creature must be the same summon. And lastly; sub02_type_01 ( 3 - CHECK_ITEM_ID ) means an item with id ( 710001 - Soul Catalyst ) must also be present in the combination","title":"MixResource Documentation"},{"location":"documentation/mixresource-doc/#description","text":"This document will explain use cases for the dbo.MixResource table and rdb.","title":"Description"},{"location":"documentation/mixresource-doc/#mix-type","text":"MIX_ENHANCE = 101, MIX_ENHANCE_SKILL_CARD = 102, MIX_ENHANCE_WITHOUT_FAIL = 103, MIX_ENHANCE_CREATURE_CARD = 104, MIX_ENHANCE_CREATURE_CARD_WITH_JOKER = 105, MIX_SET_LEVEL = 201, MIX_SET_LEVEL_CREATE_ITEM = 202, MIX_SET_LEVEL_SET_FLAG = 211, MIX_SET_LEVEL_SET_FLAG_CREATE_ITEM = 212, MIX_SET_LEVEL_SET_FLAG_CREATE_ITEM_WITH_MAIN_MATERIAL_LEVEL = 213, MIX_SET_LEVEL_ON_SUB_MATERIAL_LEVEL_SET_FLAG = 214, MIX_SET_LEVEL_SET_FLAG_CREATE_ITEM_WITH_MAIN_MATERIAL_LEVEL_SET_ZERO = 215, MIX_ADD_LEVEL = 301, MIX_ADD_LEVEL_CREATE_ITEM = 302, MIX_ADD_LEVEL_SET_FLAG = 311, MIX_ADD_LEVEL_SET_FLAG_CREATE_ITEM = 312, MIX_RECYCLE = 401, MIX_RECYCLE_ENHANCE = 402, MIX_RESTORE_ENHANCE_SET_FLAG = 501, MIX_CREATE_ITEM = 601, MIX_REPLACE_WITH = 602, MIX_SET_ELEMENTAL_EFFECT = 701, MIX_SET_ELEMENTAL_EFFECT_PARAMETER = 702, MIX_SET_SOCKET = 703, MIX_SACRIFICE_ITEM_FOR_ETHEREAL_DURABILITY_WITH_MESSAGE = 801, MIX_TRANSMIT_ETHEREAL_DURABILITY = 802, MIX_RECOVER_EXHAUSTED_ETHEREAL_DURABILITY = 803, MIX_SACRIFICE_ITEM_FOR_ETHEREAL_DURABILITY = 804, MIX_SACRIFICE_ITEM_FOR_ETHEREAL_STONE_DURABILITY = 805, MIX_TRANSMIT_ETHEREAL_DURABILITY_FROM_ETHEREAL_STONE = 806,","title":"Mix Type"},{"location":"documentation/mixresource-doc/#check-type","text":"The check enumeration is typically used in the sub0x_type_0x depending on the sub_material_count CHECK_ITEM_GROUP = 1, CHECK_ITEM_CLASS = 2, CHECK_ITEM_ID = 3, CHECK_ITEM_RANK = 4, CHECK_ITEM_LEVEL = 5, CHECK_FLAG_ON = 6, CHECK_FLAG_OFF = 7, CHECK_ENHANCE_MATCH = 8, CHECK_ENHANCE_DISMATCH = 9, CHECK_ITEM_COUNT = 10, CHECK_ELEMENTAL_EFFECT_MATCH = 11, CHECK_ELEMENTAL_EFFECT_MISMATCH = 12, CHECK_ITEM_WEAR_POSITION_MATCH = 13, CHECK_ITEM_WEAR_POSITION_MISMATCH = 14, CHECK_ITEM_COUNT_GE = 15, // The number of items exceeds the specified quantity CHECK_ITEM_ETHEREAL_DURABILITY_E = 16, // Ethereal durability is the same as the specified value CHECK_ITEM_ETHEREAL_DURABILITY_NE = 17, // Ethereal durability is different from the specified value CHECK_ITEM_GRADE = 18, // Item grade is same as specified value CHECK_SAME_ITEM_ID = 19, // Item code identical to the item in the designated slot (auxiliary material cross-reference type) CHECK_SAME_SUMMON_CODE = 20, // Minion code identical to the item in the designated slot (code material cross-reference type) CHECK_ITEM_EXPIRED_TIME_GE = 21, // The remaining time of the fixed-term item is greater than or equal to the specified value CHECK_ITEM_EXPIRED_TIME_LE = 22, // The remaining time of the fixed-term item is less than or equal to the specified value CHECK_ITEM_FIRST_SOCKET_CODE_MATCH = 23, // The code of the first slot is the same as the specified value CHECK_SAME_ITEM_ENHANCE = 24, // same enhancement as the item in the designated slot (auxiliary material cross-reference type) CHECK_SAME_SKILL_ID = 25, // The same skill ID as the item in the specified slot (auxiliary material cross-reference type)","title":"Check Type"},{"location":"documentation/mixresource-doc/#explination-by-type","text":"Below the various types of combinations will be broken down by their type (e.g. their mix_type)","title":"Explination by Type"},{"location":"documentation/mixresource-doc/#create-item-601","text":"mix_value_01 - Result item id (the item this combination will create) mix_value_02 - Result item lv (the level of the resulting item) mix_value_03 - Percent (0-100) this combination will succeeed mix_value_04 - Result item minimum count (the minimum number of the resulting item for this combination) mix_value_05 - Result item maximum count (the maximum number of the resulting item for this combination) sub_material_count - The amount of items that must be combined to create the result item. Amount of sub0*_type0* / sub0*_value_0* fields completed/used is determined by this value. sub0*_type_0* - Value enumerated by Check Type Enum sub0*_value_0* - Depends on the matching sub0*_type_0* field before it. (See following example) Example sub01_type_01: 3 sub01_value_01: 490003 sub01_type_02: 10 sub01_value_02: 1 In the above example if we refer to the Check Type Enum we can see 3 resolves to: CHECK_ITEM_ID and 10 resolves to CHECK_ITEM_COUNT . So in the above example, the first required item/ingredient for the combination is 490003 because a sub01_type_01 of 3 means we're checking the sub01_value_01 field for an item id. We are also checking to see if there is 1 of that item because the sub01_type_02 of 10 means were checking sub01_value_02 for a count.","title":"Create Item (601)"},{"location":"documentation/mixresource-doc/#enhance-creature-card-104105","text":"Note: Type 104 is enhance without Joker / 105 is enhance with Joker mix_value_01 - EnhanceResource id used in success calculation for stage +1/5 mix_value_04 - If enhance is type 104 (without joker) and the main creature card is >= +3 then this field will contain a creature card item id (likely joker 540070 ) mix_value_06 - Maximum percentage chance that the item id in mix_value_04 will be given on an enhance failure of type 104 sub_material_count - The amount of items that must be combined to create the result item. Amount of sub0*_type0* / sub0*_value_0* fields completed/used is determined by this value. main0*_type_0* - Value enumated by Check Type Enum main0*_value_0* - Depends on the matching main0*_type_0* sub0*_type_0* - Value enumerated by Check Type Enum sub0*_value_0* - Depends on the matching sub0*_type_0* field before it. (See following example) Example (type 104) type: 104 mix_value_01: 2000 mix_value_04: 540070 mix_value_06: 4 main_type_01: 1 main_value_01: 13 main_type_02: 6 main_value_02: 31 sub01_type_01: 1 sub01_value_01: 13 sub01_type_02: 6 sub01_value_02: 31 sub01_type_03: 20 sub02_type_01: 3 sub02_value_01: 710001 In the above example, we can tell we are enhancing a creature card without a joker because the type is 104 We also notice that our chance for success (per stage 1/5) can be found in the EnhanceResource table with enhance_id 2000 We also see that since we are type 104 that if our enhance fails at or above +3 (stage 3) that there is a 4% ( mix_value_06 ) chance that a Joker ( 540070 ) will be the result. By refering to Check Type Enum we know that main_type_01 (1 - CHECK_ITEM_GROUP ) means our main creature card (item) must have group 13 ( main_value_01 ) and by checking main_type_02 ( 6 - CHECK_FLAG_ON ) we can tell that the main creature card (item) must bear flag 31 ( Tamed Pet ) We can also see that sub01_type_01/02 and sub_01_value_01/02 are the same basically telling us our second creature card must be of group 13 and bearing the 31 ( Tamed Pet ) flag. Looking at sub01_type_03 ( 20 - CHECK_SAME_SUMMON_CODE ) we can see that both the main and combined creature must be the same summon. And lastly; sub02_type_01 ( 3 - CHECK_ITEM_ID ) means an item with id ( 710001 - Soul Catalyst ) must also be present in the combination","title":"Enhance Creature Card (104/105)"},{"location":"documentation/nfmapedit-doc/","text":"Definitions NFA (nFlavor Attribute) - Contains polygons used for collision/bounding. NFC (nFlavor Local/Region) - Contains information regarding regions (such as area title, bgm, sky/fog/cloud colors as-well as region ambient, diffuse and specular lighting, weather and more) NFE ( nFlavor Event) - Contains LUA script triggers, such as on_init, on_enter, on_leave which can be used to spawn monsters/props or otherwise further quests etc. NFK ( nFlavor Camera Work) - Contains camera movement information. *(such as the opening scene after creating a new character. !!!The SFrame is coded to only recognize the three existing nfk!!!) NFL ( nFlavor Lighting) - Contains lighting (directional, omni-directional) information that can be processed through Light Map Generator to create shadows on props/terrain. NFM ( nFlavor Map) - Contains information regarding the terrain (such as height, terrain textures and static props) NFP (nFlavor Path) - Contains information regarding automated walking paths (for monster, npc etc) NFS ( nFlavor Script) - Contains information regarding monster spawns (similar to NFE) NFW (nFlavor Water) - Contains information regarding water placed on the terrain GCI/GC2 ( nFlavor Grass Colony) - Contains information regarding grass on the terrain QPF (Quest Prop File) - Contains information regarding props that can be interacted with by the player. First start When starting for the first time, you will be requested to provide a 'Resource' folder. This is the folder where a structured/unstructured client dump is located. However, with this version you can also provide a client directory (containing data.000-008) Panes Area Selection This pane allows selecting areas of the terrain (by dragging a square onto the displayed terrain) is denoted by a black and white checkered area. (Unless View > Always Show Selection is checked, the area select tool and selected area can only be used in this pane) This pane also allows the generation/testing/view/camera configuration for the given maps PVS. More information on PVS can be found here. You can find a thorough explanation of the controls in this tab below: Select Area * Select - Selected the entire loaded terrain * Deselect - Deselects currently selected terrain* Local PVS Build PVS - Generates a .pvs of the currently loaded terrain (this is for the entire loaded terrain) test PVS - Toggles PVS rendering to simulate the effect this pvs will have ingame Distance - ??? Needs testing Count - ??? Needs testing Length - ??? Needs testing Camera Offset - ??? Needs testing Position - ??? Needs testing Target - ??? Needs testing SAMPLE Point Visible - ??? Needs testing Preview - ??? Needs testing User Sample Point Enable - Toggles user based PVS points User Build PVS - Generates PVS based on user points instead of automatic selection All Delete Sample Point - Removes all user placed points Force Render - ??? Needs testing Find Hole '#'- ??? Needs testing Segment Modify Selection Prop - ??? Needs testing Selection Segment - ??? Needs testing None - ??? Needs testing Export TXT - ??? Needs testing Import TXT - ??? Needs testing View Caption Segment - ??? Needs testing View Caption Prop - ??? Needs testing Height Editor This pane allows the user to alter the height of the loaded maps terrain. This includes; Raising/Lowering/Planarizing (leveling out varying terrain heights to a more common height) and flattening (useful for creating plateaus.) You can find a thorough explanation of the controls in this tab below: Raise (F1) - Raises the terrain by the given size and pressure (intensity) Lower (F2) - Lowers the terrain by the given size and pressure (intensity) Planarize (F3) - Planarize (averages) the terrain by the given size and pressure (intensity) Ignore Palanarize Pressure (F4) - Causes the Planarization of the terrain to ignore the pressure (intensity) currently selected Flatten - Flattens the terrain (useful to create plateaus) Tile Editor This pane allows the users to lay/alter/remove textures on the loaded terrain. This includes the ability to blend up to three unique textures in a variety of blend ratios. You can find a thorough explanation of the controls in this tab below: coming soon.. Opening Maps Maps can be opened in multiple ways: Press (and holding) the left 'ALT' key, then pressing the 'F' key, then pressing the 'O' key. Click the 'File' menu and then click 'Open Map (O)' Using the 'CTRL' + 'O' key combination Provide proper 'X' and 'Y' map coordinates in the 'Information' group box 4.1 Example: m009(x)_004(y) or x:9 y:4 is Horizon 4.2 No layer is required 4.3 # of Additional Maps to Load can load adjacent maps but performance takes a giant hit, it is not recommended to load more than 1 additional map Camera Movement The camera position can be moved in a variety of ways: Keyboard \u2191 / Num. Pad 8 - Forward (x plane) \u2190 / Num. Pad 4 - Left (y plane) \u2193 / Num. Pad 3 - Backward (x plane) \u2192 / Num. Pad 6 - Right (y plane) Shift - Increases the movement speed of the camera movement through the directional keys Mouse Moving across the x / y plane Press and hold the Center / Scroll mouse wheel and drag the mouse. Panning Press and hold the L. Ctrl key and the Center / Scroll mouse wheel and drag the mouse. (to increase the camera movement speed hold the Shift key)","title":"NFMapEdit Documentation"},{"location":"documentation/nfmapedit-doc/#definitions","text":"NFA (nFlavor Attribute) - Contains polygons used for collision/bounding. NFC (nFlavor Local/Region) - Contains information regarding regions (such as area title, bgm, sky/fog/cloud colors as-well as region ambient, diffuse and specular lighting, weather and more) NFE ( nFlavor Event) - Contains LUA script triggers, such as on_init, on_enter, on_leave which can be used to spawn monsters/props or otherwise further quests etc. NFK ( nFlavor Camera Work) - Contains camera movement information. *(such as the opening scene after creating a new character. !!!The SFrame is coded to only recognize the three existing nfk!!!) NFL ( nFlavor Lighting) - Contains lighting (directional, omni-directional) information that can be processed through Light Map Generator to create shadows on props/terrain. NFM ( nFlavor Map) - Contains information regarding the terrain (such as height, terrain textures and static props) NFP (nFlavor Path) - Contains information regarding automated walking paths (for monster, npc etc) NFS ( nFlavor Script) - Contains information regarding monster spawns (similar to NFE) NFW (nFlavor Water) - Contains information regarding water placed on the terrain GCI/GC2 ( nFlavor Grass Colony) - Contains information regarding grass on the terrain QPF (Quest Prop File) - Contains information regarding props that can be interacted with by the player.","title":"Definitions"},{"location":"documentation/nfmapedit-doc/#first-start","text":"When starting for the first time, you will be requested to provide a 'Resource' folder. This is the folder where a structured/unstructured client dump is located. However, with this version you can also provide a client directory (containing data.000-008)","title":"First start"},{"location":"documentation/nfmapedit-doc/#panes","text":"","title":"Panes"},{"location":"documentation/nfmapedit-doc/#area-selection","text":"This pane allows selecting areas of the terrain (by dragging a square onto the displayed terrain) is denoted by a black and white checkered area. (Unless View > Always Show Selection is checked, the area select tool and selected area can only be used in this pane) This pane also allows the generation/testing/view/camera configuration for the given maps PVS. More information on PVS can be found here. You can find a thorough explanation of the controls in this tab below: Select Area * Select - Selected the entire loaded terrain * Deselect - Deselects currently selected terrain* Local PVS Build PVS - Generates a .pvs of the currently loaded terrain (this is for the entire loaded terrain) test PVS - Toggles PVS rendering to simulate the effect this pvs will have ingame Distance - ??? Needs testing Count - ??? Needs testing Length - ??? Needs testing Camera Offset - ??? Needs testing Position - ??? Needs testing Target - ??? Needs testing SAMPLE Point Visible - ??? Needs testing Preview - ??? Needs testing User Sample Point Enable - Toggles user based PVS points User Build PVS - Generates PVS based on user points instead of automatic selection All Delete Sample Point - Removes all user placed points Force Render - ??? Needs testing Find Hole '#'- ??? Needs testing Segment Modify Selection Prop - ??? Needs testing Selection Segment - ??? Needs testing None - ??? Needs testing Export TXT - ??? Needs testing Import TXT - ??? Needs testing View Caption Segment - ??? Needs testing View Caption Prop - ??? Needs testing","title":"Area Selection"},{"location":"documentation/nfmapedit-doc/#height-editor","text":"This pane allows the user to alter the height of the loaded maps terrain. This includes; Raising/Lowering/Planarizing (leveling out varying terrain heights to a more common height) and flattening (useful for creating plateaus.) You can find a thorough explanation of the controls in this tab below: Raise (F1) - Raises the terrain by the given size and pressure (intensity) Lower (F2) - Lowers the terrain by the given size and pressure (intensity) Planarize (F3) - Planarize (averages) the terrain by the given size and pressure (intensity) Ignore Palanarize Pressure (F4) - Causes the Planarization of the terrain to ignore the pressure (intensity) currently selected Flatten - Flattens the terrain (useful to create plateaus)","title":"Height Editor"},{"location":"documentation/nfmapedit-doc/#tile-editor","text":"This pane allows the users to lay/alter/remove textures on the loaded terrain. This includes the ability to blend up to three unique textures in a variety of blend ratios. You can find a thorough explanation of the controls in this tab below: coming soon..","title":"Tile Editor"},{"location":"documentation/nfmapedit-doc/#opening-maps","text":"Maps can be opened in multiple ways: Press (and holding) the left 'ALT' key, then pressing the 'F' key, then pressing the 'O' key. Click the 'File' menu and then click 'Open Map (O)' Using the 'CTRL' + 'O' key combination Provide proper 'X' and 'Y' map coordinates in the 'Information' group box 4.1 Example: m009(x)_004(y) or x:9 y:4 is Horizon 4.2 No layer is required 4.3 # of Additional Maps to Load can load adjacent maps but performance takes a giant hit, it is not recommended to load more than 1 additional map","title":"Opening Maps"},{"location":"documentation/nfmapedit-doc/#camera-movement","text":"The camera position can be moved in a variety of ways:","title":"Camera Movement"},{"location":"documentation/nfmapedit-doc/#keyboard","text":"\u2191 / Num. Pad 8 - Forward (x plane) \u2190 / Num. Pad 4 - Left (y plane) \u2193 / Num. Pad 3 - Backward (x plane) \u2192 / Num. Pad 6 - Right (y plane) Shift - Increases the movement speed of the camera movement through the directional keys","title":"Keyboard"},{"location":"documentation/nfmapedit-doc/#mouse","text":"Moving across the x / y plane Press and hold the Center / Scroll mouse wheel and drag the mouse. Panning Press and hold the L. Ctrl key and the Center / Scroll mouse wheel and drag the mouse. (to increase the camera movement speed hold the Shift key)","title":"Mouse"},{"location":"documentation/petresource-doc/","text":"PetResource Documentation cage_id The dbo.ItemResource id of the item used to summon the Pet Notes The cage item must present status_flag 32 The cage item must present opt_type_0 90 type The type field is enumerated below: CREATURE_ETC = 0; CREATURE_BEAST = 1; CREATURE_SEMIHUMAN = 2; CREATURE_ELEMENTAL = 3; CREATURE_ANGEL = 4; CREATURE_DEVIL = 5; CREATURE_MECHA = 6; CREATURE_DRAGON = 7; CREATURE_UNDEAD = 8; CREATURE_HUMAN = 9; CREATURE_ALL = 99; rate/attribute_Flag A bit_flag field used determine some behaviors of the pet. Note: in ep7.2 the field is named rate in of ep9+ the field is named attribute_flag RATE_RARE = 1 << 0, // Rare or not RATE_SHOVELABLE = 1 << 1, // Whether shoveling is possible RATE_ITEM_COLLECTABLE = 1 << 2, // Whether it is possible to automatically pick up items RATE_WEIGHT_1000 = 1 << 3, // Increase possession (1000) (ep 9.x) RATE_WEIGHT_2000 = 1 << 4, // Increase possession (2000) (ep 9.x) RATE_WEIGHT_4000 = 1 << 5, // Increase possession (4000) (ep 9.x) RATE_WEIGHT_8000 = 1 << 6, // Increase possession (8000) (ep 9.x) Note: in ep7.2 the value is always 5 which enumerates to the following flags: RATE_RARE = 1 << 0, // Rare or not RATE_ITEM_COLLECTABLE = 1 << 2, // Whether it is possible to automatically pick up items Note: newer pets such as Boss pets in ep9+ use 21 which enumerates to the following flags: RATE_RARE = 1 << 0, // Rare or not RATE_ITEM_COLLECTABLE = 1 << 2, // Whether it is possible to automatically pick up items RATE_WEIGHT_2000 = 1 << 4, // Increase possession (2000) (ep 9.x) model The filename of the model this pet will use to be rendered e.g. pet_arcadia motion_file_id This id links to the MonsterMotionSet table/rdb and must link to a vlid id or the pet will be invisible when summoned. Item Collection In order for the Collect Items skill to be properly displayed for a summoned pet an entry needs to be created in the SkillTreeResource table/rdb with the pet's id as the job_id and the skill_id as 46018 (All other fields may be 0 )","title":"PetResoure Documentation"},{"location":"documentation/petresource-doc/#petresource-documentation","text":"","title":"PetResource Documentation"},{"location":"documentation/petresource-doc/#cage_id","text":"The dbo.ItemResource id of the item used to summon the Pet Notes The cage item must present status_flag 32 The cage item must present opt_type_0 90","title":"cage_id"},{"location":"documentation/petresource-doc/#type","text":"The type field is enumerated below: CREATURE_ETC = 0; CREATURE_BEAST = 1; CREATURE_SEMIHUMAN = 2; CREATURE_ELEMENTAL = 3; CREATURE_ANGEL = 4; CREATURE_DEVIL = 5; CREATURE_MECHA = 6; CREATURE_DRAGON = 7; CREATURE_UNDEAD = 8; CREATURE_HUMAN = 9; CREATURE_ALL = 99;","title":"type"},{"location":"documentation/petresource-doc/#rateattribute_flag","text":"A bit_flag field used determine some behaviors of the pet. Note: in ep7.2 the field is named rate in of ep9+ the field is named attribute_flag RATE_RARE = 1 << 0, // Rare or not RATE_SHOVELABLE = 1 << 1, // Whether shoveling is possible RATE_ITEM_COLLECTABLE = 1 << 2, // Whether it is possible to automatically pick up items RATE_WEIGHT_1000 = 1 << 3, // Increase possession (1000) (ep 9.x) RATE_WEIGHT_2000 = 1 << 4, // Increase possession (2000) (ep 9.x) RATE_WEIGHT_4000 = 1 << 5, // Increase possession (4000) (ep 9.x) RATE_WEIGHT_8000 = 1 << 6, // Increase possession (8000) (ep 9.x) Note: in ep7.2 the value is always 5 which enumerates to the following flags: RATE_RARE = 1 << 0, // Rare or not RATE_ITEM_COLLECTABLE = 1 << 2, // Whether it is possible to automatically pick up items Note: newer pets such as Boss pets in ep9+ use 21 which enumerates to the following flags: RATE_RARE = 1 << 0, // Rare or not RATE_ITEM_COLLECTABLE = 1 << 2, // Whether it is possible to automatically pick up items RATE_WEIGHT_2000 = 1 << 4, // Increase possession (2000) (ep 9.x)","title":"rate/attribute_Flag"},{"location":"documentation/petresource-doc/#model","text":"The filename of the model this pet will use to be rendered e.g. pet_arcadia","title":"model"},{"location":"documentation/petresource-doc/#motion_file_id","text":"This id links to the MonsterMotionSet table/rdb and must link to a vlid id or the pet will be invisible when summoned.","title":"motion_file_id"},{"location":"documentation/petresource-doc/#item-collection","text":"In order for the Collect Items skill to be properly displayed for a summoned pet an entry needs to be created in the SkillTreeResource table/rdb with the pet's id as the job_id and the skill_id as 46018 (All other fields may be 0 )","title":"Item Collection"},{"location":"guides/mixcategory-guide/","text":"Editing MixCategoryResource In-Game Structure Column 1 - high_category_id Column 2 - middle_category_id Column 3 - low_category_id Each column is a list of strings and the ***_category_id is the associated index (position) of this entry. Consider the following example: Which consists of the following indexes: high_category_id = 0 middle_category_id = 0 low_category_id = -1 While the following example: consist of the following indexes: high_category_id = 0 middle_category_id = 0 low_category_id = 0 In the above example, all the ids are 0 because each category selected is at the beginning of its list) However in the following example: the selected ***_category_id would be: high_category_id = 1 middle_category_id = 0 low_category_id = 0 Database Structure Notes: MixCategoryResource services both the Combine and Enhance formula windows. See mix_id -1 in _category_id means there is nothing to be displayed. Columns id = A unique identifier mix_id = 1 Formula / 0 Enhance local_flag = Determines which regions this formula can be displayed to. category_text_id = dbo.StringResource.[code] of string to be displayed as category text formal_text_id = dbo.StringResource.[code] of string to be displayed as ingredients result_text_id = dbo.StringResource.[code] of string to be displayed as the result","title":"Mix Category Guide"},{"location":"guides/mixcategory-guide/#editing-mixcategoryresource","text":"","title":"Editing MixCategoryResource"},{"location":"guides/mixcategory-guide/#in-game-structure","text":"Column 1 - high_category_id Column 2 - middle_category_id Column 3 - low_category_id Each column is a list of strings and the ***_category_id is the associated index (position) of this entry. Consider the following example: Which consists of the following indexes: high_category_id = 0 middle_category_id = 0 low_category_id = -1 While the following example: consist of the following indexes: high_category_id = 0 middle_category_id = 0 low_category_id = 0 In the above example, all the ids are 0 because each category selected is at the beginning of its list) However in the following example: the selected ***_category_id would be: high_category_id = 1 middle_category_id = 0 low_category_id = 0","title":"In-Game Structure"},{"location":"guides/mixcategory-guide/#database-structure","text":"Notes: MixCategoryResource services both the Combine and Enhance formula windows. See mix_id -1 in _category_id means there is nothing to be displayed.","title":"Database Structure"},{"location":"guides/mixcategory-guide/#columns","text":"id = A unique identifier mix_id = 1 Formula / 0 Enhance local_flag = Determines which regions this formula can be displayed to. category_text_id = dbo.StringResource.[code] of string to be displayed as category text formal_text_id = dbo.StringResource.[code] of string to be displayed as ingredients result_text_id = dbo.StringResource.[code] of string to be displayed as the result","title":"Columns"},{"location":"guides/server-setup/","text":"test","title":"Server setup"},{"location":"releases/grimoire-rel/","text":"Description Grimoire is a powerful multi-purpose Rappelz asset management suite, written in c# by iSmokeDrow with nearly 5 years of active development. Built on top of libraries like: DataCore (Data.XXX Management) Daedalus (RDB Management) It can deliver an easy and fast user experience. Version 4.11.1 Features General In-Depth settings menu Seemless and powerful tab style management Easy and fast new tab selection/creation Configurable Styles List (Advanced users ONLY!) Selectable Default Style Powerful Database configuration Quick and helpful keycombos CTRL + O (Open File) [RDB/DATA] CTRL + S (Save File) [RDB] CTRL + F (Search) [RDB] CTRL + N (New Tab, same type as currently loaded) [ALL] SHIFT + N (New Tab, if Data is loaded) [DATA] CTRL + R (Rebuild Client) [DATA] Data.XXX Management File Backups Load File > Load Drop file onto clean Data Tab Definable Default Directory View client file index View detailed information by clicking on a filename View detailed information about a given extension by opening its node in the right-hand drop tree view Filter viewed index by clicking a particular extension Export All By extension By selected in grid (single/multiple) Insert Single/Multiple Drag-N-Drop file'(s) onto loaded Client file index Create New Client (from Export > All dump) Option to clear Data tab on successful creation or load created client to grid Search All In Filtered Index (by extension) Compare (stored with external) Rebuild via Rebuild Wizard View Current Size | File Count | Fragmentation % (per data.xxx or all) View visual presentation of Size vs Fragmentation via Pie Chart Statistics and Pie Chart update in real time during rebuild RDB Editing User defined structures [LUA via MoonSharp] Easy to use FieldTypes Easy to use flags per field definition Definable header Definable fileName/tableName ProcessRow functionality for indepth load/save requirements Definable selectStatement/sqlColumns for up/down patching across different epics Easy-to-use fast drop-down structures list (populated on tab creation) Drop-Down select Type with auto-completed (press enter to load) Load From File By File menu By dropping file onto clean RDB Tab From SQL From Data (Data.XXX) Definable Default Directory Save To File As encrypted or plain name With or without (ascii) To SQL Table Ability to backup/script schema only/schema and data in scripts To CSV file To SQL file Ability to Sort visible colums (ascending/descending) by clicking the column header Ability to Search any loaded column File Hashing Hash name in realtime w/flip option Add files to grid By right clicking grid Dropping File/Folder onto hasher tab grid Add/Remove (ascii) during add Auto-Conversion Auto-Clear tab after successful conversion Flag Utility Customizable flag lists Switch flag list easy Covers multiple bit flag fields like stateresource buff opt_var stateresource state_time_type itemresource item_use_flag Auto convert Calculate AND Reverse calculations Documentation Grimoire DataCore Daedalus Download v4.11.1","title":"Grimoire v4.11.1"},{"location":"releases/grimoire-rel/#description","text":"Grimoire is a powerful multi-purpose Rappelz asset management suite, written in c# by iSmokeDrow with nearly 5 years of active development. Built on top of libraries like: DataCore (Data.XXX Management) Daedalus (RDB Management) It can deliver an easy and fast user experience.","title":"Description"},{"location":"releases/grimoire-rel/#version","text":"4.11.1","title":"Version"},{"location":"releases/grimoire-rel/#features","text":"","title":"Features"},{"location":"releases/grimoire-rel/#general","text":"In-Depth settings menu Seemless and powerful tab style management Easy and fast new tab selection/creation Configurable Styles List (Advanced users ONLY!) Selectable Default Style Powerful Database configuration Quick and helpful keycombos CTRL + O (Open File) [RDB/DATA] CTRL + S (Save File) [RDB] CTRL + F (Search) [RDB] CTRL + N (New Tab, same type as currently loaded) [ALL] SHIFT + N (New Tab, if Data is loaded) [DATA] CTRL + R (Rebuild Client) [DATA]","title":"General"},{"location":"releases/grimoire-rel/#dataxxx-management","text":"File Backups Load File > Load Drop file onto clean Data Tab Definable Default Directory View client file index View detailed information by clicking on a filename View detailed information about a given extension by opening its node in the right-hand drop tree view Filter viewed index by clicking a particular extension Export All By extension By selected in grid (single/multiple) Insert Single/Multiple Drag-N-Drop file'(s) onto loaded Client file index Create New Client (from Export > All dump) Option to clear Data tab on successful creation or load created client to grid Search All In Filtered Index (by extension) Compare (stored with external) Rebuild via Rebuild Wizard View Current Size | File Count | Fragmentation % (per data.xxx or all) View visual presentation of Size vs Fragmentation via Pie Chart Statistics and Pie Chart update in real time during rebuild","title":"Data.XXX Management"},{"location":"releases/grimoire-rel/#rdb-editing","text":"User defined structures [LUA via MoonSharp] Easy to use FieldTypes Easy to use flags per field definition Definable header Definable fileName/tableName ProcessRow functionality for indepth load/save requirements Definable selectStatement/sqlColumns for up/down patching across different epics Easy-to-use fast drop-down structures list (populated on tab creation) Drop-Down select Type with auto-completed (press enter to load) Load From File By File menu By dropping file onto clean RDB Tab From SQL From Data (Data.XXX) Definable Default Directory Save To File As encrypted or plain name With or without (ascii) To SQL Table Ability to backup/script schema only/schema and data in scripts To CSV file To SQL file Ability to Sort visible colums (ascending/descending) by clicking the column header Ability to Search any loaded column","title":"RDB Editing"},{"location":"releases/grimoire-rel/#file-hashing","text":"Hash name in realtime w/flip option Add files to grid By right clicking grid Dropping File/Folder onto hasher tab grid Add/Remove (ascii) during add Auto-Conversion Auto-Clear tab after successful conversion","title":"File Hashing"},{"location":"releases/grimoire-rel/#flag-utility","text":"Customizable flag lists Switch flag list easy Covers multiple bit flag fields like stateresource buff opt_var stateresource state_time_type itemresource item_use_flag Auto convert Calculate AND Reverse calculations","title":"Flag Utility"},{"location":"releases/grimoire-rel/#documentation","text":"Grimoire DataCore Daedalus","title":"Documentation"},{"location":"releases/grimoire-rel/#download","text":"v4.11.1","title":"Download"},{"location":"releases/removeit-rel/","text":"Description Maintenance sql script to clear the Telecaster of deleted characters (aswell as their assets and pets) Script USE Telecaster -- Change my name to your Telecaster name DECLARE @CUR INT, @MAX INT, @SID INT, @NAME NVARCHAR(50), @PETCUR INT, @PETMAX INT, @PETSID INT SET @CUR = 0 SET @PETCUR = 0 SET @MAX = (SELECT COUNT(*) FROM dbo.Character WHERE name LIKE '%@%') PRINT CONCAT(N'There are a total of ', @MAX, N' deleted characters') WHILE @CUR < @MAX BEGIN SET @SID = (SELECT TOP(1) sid FROM dbo.Character WHERE name LIKE '%@%') SET @NAME = (SELECT TOP(1) name FROM dbo.Character WHERE sid = @SID) PRINT CONCAT(N'Processing entry ', @CUR, N' of ', @MAX, N' with SID: ', @SID, N' and NAME: ', @NAME) DELETE FROM dbo.Character WHERE sid = @SID DELETE FROM dbo.Auction WHERE seller_id = @SID DELETE FROM dbo.Denials WHERE owner_id = @NAME OR denial_id = @NAME DELETE FROM dbo.EventAreaEnterCount WHERE player_id = @SID DELETE FROM dbo.Farm WHERE owner_id = @SID DELETE FROM dbo.Favor WHERE owner_id = @SID DELETE FROM dbo.Friends WHERE owner_id = @NAME or friend_id = @NAME DELETE FROM dbo.GuildMember WHERE player_id = @SID DELETE FROM dbo.Item WHERE owner_id = @SID DELETE FROM dbo.ItemCoolTime WHERE owner_id = @SID DELETE FROM dbo.ItemKeeping WHERE owner_id = @SID DELETE FROM dbo.Party WHERE leader_id = @SID DELETE FROM dbo.Quest WHERE owner_id = @SID DELETE FROM dbo.QuestCoolTime WHERE owner_id = @SID DELETE FROM dbo.RankingScore WHERE owner_id = @SID DELETE FROM dbo.Skill WHERE owner_id = @SID DELETE FROM dbo.State WHERE owner_id = @SID DELETE FROM dbo.Title WHERE owner_id = @SID DELETE FROM dbo.TitleCondition WHERE owner_id = @SID SET @PETMAX = (SELECT COUNT(*) FROM dbo.Summon WHERE owner_id = @SID) PRINT CONCAT(@PETMAX, N' Pets detected for this character.') WHILE @PETCUR < @PETMAX BEGIN SET @PETSID = (SELECT TOP(1) sid FROM dbo.Summon WHERE owner_id = @SID) DELETE FROM dbo.Summon WHERE sid = @PETSID DELETE FROM dbo.Item WHERE summon_id = @PETSID DELETE FROM dbo.Skill WHERE summon_id = @PETSID DELETE FROM dbo.State WHERE summon_id = @PETSID SET @PETCUR = @PETCUR + 1 PRINT CONCAT(N' Pet bearing sid ', @PETSID, N' and all related information deleted.') END PRINT N'Delete Successful' SET @CUR = @CUR + 1 END Usage Launch SSMS Use CTRL+N key combo Paste the script Edit USE statement as needed Execute","title":"RemoveIT Script"},{"location":"releases/removeit-rel/#description","text":"Maintenance sql script to clear the Telecaster of deleted characters (aswell as their assets and pets)","title":"Description"},{"location":"releases/removeit-rel/#script","text":"USE Telecaster -- Change my name to your Telecaster name DECLARE @CUR INT, @MAX INT, @SID INT, @NAME NVARCHAR(50), @PETCUR INT, @PETMAX INT, @PETSID INT SET @CUR = 0 SET @PETCUR = 0 SET @MAX = (SELECT COUNT(*) FROM dbo.Character WHERE name LIKE '%@%') PRINT CONCAT(N'There are a total of ', @MAX, N' deleted characters') WHILE @CUR < @MAX BEGIN SET @SID = (SELECT TOP(1) sid FROM dbo.Character WHERE name LIKE '%@%') SET @NAME = (SELECT TOP(1) name FROM dbo.Character WHERE sid = @SID) PRINT CONCAT(N'Processing entry ', @CUR, N' of ', @MAX, N' with SID: ', @SID, N' and NAME: ', @NAME) DELETE FROM dbo.Character WHERE sid = @SID DELETE FROM dbo.Auction WHERE seller_id = @SID DELETE FROM dbo.Denials WHERE owner_id = @NAME OR denial_id = @NAME DELETE FROM dbo.EventAreaEnterCount WHERE player_id = @SID DELETE FROM dbo.Farm WHERE owner_id = @SID DELETE FROM dbo.Favor WHERE owner_id = @SID DELETE FROM dbo.Friends WHERE owner_id = @NAME or friend_id = @NAME DELETE FROM dbo.GuildMember WHERE player_id = @SID DELETE FROM dbo.Item WHERE owner_id = @SID DELETE FROM dbo.ItemCoolTime WHERE owner_id = @SID DELETE FROM dbo.ItemKeeping WHERE owner_id = @SID DELETE FROM dbo.Party WHERE leader_id = @SID DELETE FROM dbo.Quest WHERE owner_id = @SID DELETE FROM dbo.QuestCoolTime WHERE owner_id = @SID DELETE FROM dbo.RankingScore WHERE owner_id = @SID DELETE FROM dbo.Skill WHERE owner_id = @SID DELETE FROM dbo.State WHERE owner_id = @SID DELETE FROM dbo.Title WHERE owner_id = @SID DELETE FROM dbo.TitleCondition WHERE owner_id = @SID SET @PETMAX = (SELECT COUNT(*) FROM dbo.Summon WHERE owner_id = @SID) PRINT CONCAT(@PETMAX, N' Pets detected for this character.') WHILE @PETCUR < @PETMAX BEGIN SET @PETSID = (SELECT TOP(1) sid FROM dbo.Summon WHERE owner_id = @SID) DELETE FROM dbo.Summon WHERE sid = @PETSID DELETE FROM dbo.Item WHERE summon_id = @PETSID DELETE FROM dbo.Skill WHERE summon_id = @PETSID DELETE FROM dbo.State WHERE summon_id = @PETSID SET @PETCUR = @PETCUR + 1 PRINT CONCAT(N' Pet bearing sid ', @PETSID, N' and all related information deleted.') END PRINT N'Delete Successful' SET @CUR = @CUR + 1 END","title":"Script"},{"location":"releases/removeit-rel/#usage","text":"Launch SSMS Use CTRL+N key combo Paste the script Edit USE statement as needed Execute","title":"Usage"}]}